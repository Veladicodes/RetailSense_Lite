{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4772bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:58.719853Z",
     "iopub.status.busy": "2025-11-01T17:46:58.719402Z",
     "iopub.status.idle": "2025-11-01T17:47:01.889130Z",
     "shell.execute_reply": "2025-11-01T17:47:01.888358Z"
    },
    "papermill": {
     "duration": 3.183715,
     "end_time": "2025-11-01T17:47:01.891432",
     "exception": false,
     "start_time": "2025-11-01T17:46:58.707717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Phase 3 outputs cleared\n",
      "üöÄ RETAILSENSE PHASE 3: BUSINESS LAYER & INSIGHTS\n",
      "============================================================\n",
      "üìÖ Started: 2025-11-01 23:17:01\n",
      "üéØ Goal: Convert ML predictions & anomalies into actionable business insights\n",
      "üìÇ Processed Data Path: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "üìÇ Outputs Directory: F:\\RetailSense_Lite\\outputs\n",
      "üìä Dashboard CSVs will be saved at:\n",
      "  - F:\\RetailSense_Lite\\outputs\\business_inventory_alerts.csv\n",
      "  - F:\\RetailSense_Lite\\outputs\\business_seasonal_insights.csv\n",
      "  - F:\\RetailSense_Lite\\outputs\\business_pricing_opportunities.csv\n",
      "  - F:\\RetailSense_Lite\\outputs\\business_sales_anomalies.csv\n",
      "  - F:\\RetailSense_Lite\\outputs\\phase3_completion_report.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 1: Setup and Imports (Phase 3) + Upload Gate\n",
    "# -------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ------------------- Project Paths -------------------\n",
    "PROJECT_ROOT = r\"F:\\RetailSense_Lite\"\n",
    "DATA_PROCESSED_PATH = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"data_with_all_features.csv\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "UPLOAD_DIR = os.path.join(PROJECT_ROOT, \"data\", \"uploaded\")\n",
    "DEFAULT_UPLOAD_PATH = os.path.join(UPLOAD_DIR, \"uploaded_data.csv\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------- Upload Gate -------------------\n",
    "UPLOADED_DATA_PATH = os.getenv('UPLOADED_DATA_PATH')\n",
    "if not UPLOADED_DATA_PATH or not UPLOADED_DATA_PATH.strip():\n",
    "    UPLOADED_DATA_PATH = DEFAULT_UPLOAD_PATH\n",
    "UPLOADED_DATA_PATH = os.path.normpath(UPLOADED_DATA_PATH)\n",
    "\n",
    "SKIP_PHASE3 = False\n",
    "\n",
    "def clear_phase3_outputs():\n",
    "    try:\n",
    "        for fname in [\n",
    "            os.path.join(OUTPUT_DIR, \"business_inventory_alerts.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"business_seasonal_insights.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"business_pricing_opportunities.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"business_sales_anomalies.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"phase3_completion_report.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"executive_dashboard.png\"),\n",
    "        ]:\n",
    "            if os.path.exists(fname):\n",
    "                os.remove(fname)\n",
    "        print(\"üßπ Phase 3 outputs cleared\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not clear some Phase 3 outputs: {e}\")\n",
    "\n",
    "if not os.path.exists(UPLOADED_DATA_PATH):\n",
    "    clear_phase3_outputs()\n",
    "    SKIP_PHASE3 = True\n",
    "    print(\"‚ùå No uploaded data detected. Phase 3 will be skipped to avoid stale results.\")\n",
    "    print(\"   Set 'UPLOADED_DATA_PATH' or place file at: \" + DEFAULT_UPLOAD_PATH)\n",
    "else:\n",
    "    clear_phase3_outputs()\n",
    "\n",
    "# ------------------- Startup Info -------------------\n",
    "print(\"üöÄ RETAILSENSE PHASE 3: BUSINESS LAYER & INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÖ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"üéØ Goal: Convert ML predictions & anomalies into actionable business insights\")\n",
    "print(f\"üìÇ Processed Data Path: {DATA_PROCESSED_PATH}\")\n",
    "print(f\"üìÇ Outputs Directory: {OUTPUT_DIR}\")\n",
    "print(f\"üìä Dashboard CSVs will be saved at:\\n\"\n",
    "      f\"  - {os.path.join(OUTPUT_DIR, 'business_inventory_alerts.csv')}\\n\"\n",
    "      f\"  - {os.path.join(OUTPUT_DIR, 'business_seasonal_insights.csv')}\\n\"\n",
    "      f\"  - {os.path.join(OUTPUT_DIR, 'business_pricing_opportunities.csv')}\\n\"\n",
    "      f\"  - {os.path.join(OUTPUT_DIR, 'business_sales_anomalies.csv')}\\n\"\n",
    "      f\"  - {os.path.join(OUTPUT_DIR, 'phase3_completion_report.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39570279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:01.910055Z",
     "iopub.status.busy": "2025-11-01T17:47:01.909564Z",
     "iopub.status.idle": "2025-11-01T17:47:01.936712Z",
     "shell.execute_reply": "2025-11-01T17:47:01.935937Z"
    },
    "papermill": {
     "duration": 0.037245,
     "end_time": "2025-11-01T17:47:01.938137",
     "exception": false,
     "start_time": "2025-11-01T17:47:01.900892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è business_insights.py not found, using fallback engine...\n",
      "‚úÖ Data loaded successfully: 525 rows, 67 columns\n",
      "‚úÖ Data loaded from: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 2: Import Business Insights Engine (Guarded)\n",
    "# -------------------\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path for imports\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping Phase 3 imports/load (no uploaded data).\")\n",
    "    engine = None\n",
    "    loaded = False\n",
    "else:\n",
    "    try:\n",
    "        from business_insights import BusinessInsightsEngine\n",
    "        print(\"‚úÖ Imported BusinessInsightsEngine from business_insights.py\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è business_insights.py not found, using fallback engine...\")\n",
    "\n",
    "        class BusinessInsightsEngine:\n",
    "            def __init__(self):\n",
    "                self.df = None\n",
    "\n",
    "            def load_data(self, data_path):\n",
    "                try:\n",
    "                    self.df = pd.read_csv(data_path)\n",
    "                    if 'week_start' in self.df.columns:\n",
    "                        self.df['week_start'] = pd.to_datetime(self.df['week_start'])\n",
    "                    print(f\"‚úÖ Data loaded successfully: {self.df.shape[0]} rows, {self.df.shape[1]} columns\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading data: {e}\")\n",
    "                    return False\n",
    "\n",
    "            def preview(self, n=5):\n",
    "                if self.df is not None:\n",
    "                    print(self.df.head(n))\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Data not loaded yet.\")\n",
    "\n",
    "    engine = BusinessInsightsEngine()\n",
    "    loaded = engine.load_data(DATA_PROCESSED_PATH)\n",
    "    if not loaded:\n",
    "        print(f\"‚ùå Failed to load data from: {DATA_PROCESSED_PATH}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Data loaded from: {DATA_PROCESSED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1783feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:01.956546Z",
     "iopub.status.busy": "2025-11-01T17:47:01.956113Z",
     "iopub.status.idle": "2025-11-01T17:47:01.977209Z",
     "shell.execute_reply": "2025-11-01T17:47:01.976402Z"
    },
    "papermill": {
     "duration": 0.032536,
     "end_time": "2025-11-01T17:47:01.978581",
     "exception": false,
     "start_time": "2025-11-01T17:47:01.946045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LOADING PHASE 2 DATA FOR BUSINESS INSIGHTS...\n",
      "--------------------------------------------------\n",
      "‚úÖ Successfully loaded: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "üìä Data shape: (525, 67)\n",
      "‚úÖ All required columns for Phase 3 are present\n",
      "\n",
      "üìà DATA SUMMARY:\n",
      "   ‚Ä¢ Time Range: 2023-10-30 00:00:00 ‚Üí 2025-10-27 00:00:00\n",
      "   ‚Ä¢ Total Products: 5\n",
      "   ‚Ä¢ Total Categories: 5\n",
      "   ‚Ä¢ Total Records: 525\n",
      "‚úÖ Phase 2 data ready for business insights processing.\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 3: Load and Validate Phase 3 Data (Guarded)\n",
    "# -------------------\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping Phase 3 data load (no uploaded data).\")\n",
    "    df = None\n",
    "else:\n",
    "    def load_phase3_data():\n",
    "        print(\"\\nüìä LOADING PHASE 2 DATA FOR BUSINESS INSIGHTS...\")\n",
    "        print(\"-\" * 50)\n",
    "        df = None\n",
    "        \n",
    "        # CRITICAL: Check if data_with_all_features.csv exists\n",
    "        if not os.path.exists(DATA_PROCESSED_PATH):\n",
    "            print(f\"‚ùå CRITICAL ERROR: File not found: {DATA_PROCESSED_PATH}\")\n",
    "            print(\"‚ö†Ô∏è  This file should be created by Phase 2.\")\n",
    "            print(\"‚ö†Ô∏è  Phase 3 CANNOT run with old/stale data.\")\n",
    "            print(\"‚ö†Ô∏è  Please ensure Phase 2 completed successfully before running Phase 3.\")\n",
    "            return None\n",
    "        \n",
    "        # Check file freshness - it should be recent (within last hour)\n",
    "        try:\n",
    "            from datetime import datetime, timedelta\n",
    "            file_mtime = datetime.fromtimestamp(os.path.getmtime(DATA_PROCESSED_PATH))\n",
    "            time_diff = datetime.now() - file_mtime\n",
    "            \n",
    "            if time_diff > timedelta(hours=1):\n",
    "                print(f\"‚ö†Ô∏è  WARNING: Data file is {time_diff} old!\")\n",
    "                print(f\"‚ö†Ô∏è  File last modified: {file_mtime}\")\n",
    "                print(f\"‚ö†Ô∏è  This may be stale data from a previous run.\")\n",
    "                print(f\"‚ö†Ô∏è  Recommended: Run Phase 1 and Phase 2 first to generate fresh data.\")\n",
    "                # Don't fail, but warn heavily\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not check file age: {e}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(DATA_PROCESSED_PATH)\n",
    "            if 'week_start' in df.columns:\n",
    "                df['week_start'] = pd.to_datetime(df['week_start'])\n",
    "            print(f\"‚úÖ Successfully loaded: {DATA_PROCESSED_PATH}\")\n",
    "            print(f\"üìä Data shape: {df.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå File not found: {DATA_PROCESSED_PATH}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {DATA_PROCESSED_PATH}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        required_cols = [\n",
    "            'product_name', 'category', 'sales_qty', 'stock_on_hand', 'price',\n",
    "            'week_start', 'promotion', 'holiday_flag'\n",
    "        ]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"‚úÖ All required columns for Phase 3 are present\")\n",
    "        \n",
    "        print(f\"\\nüìà DATA SUMMARY:\")\n",
    "        if 'week_start' in df.columns:\n",
    "            print(f\"   ‚Ä¢ Time Range: {df['week_start'].min()} ‚Üí {df['week_start'].max()}\")\n",
    "        if 'product_name' in df.columns:\n",
    "            print(f\"   ‚Ä¢ Total Products: {df['product_name'].nunique()}\")\n",
    "        if 'category' in df.columns:\n",
    "            print(f\"   ‚Ä¢ Total Categories: {df['category'].nunique()}\")\n",
    "        print(f\"   ‚Ä¢ Total Records: {len(df)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    df = load_phase3_data()\n",
    "    if df is None:\n",
    "        print(\"‚ùå Phase 2 data load failed. Exiting Phase 3 pipeline.\")\n",
    "        print(\"üõë Phase 3 will NOT proceed with stale/missing data.\")\n",
    "        SKIP_PHASE3 = True\n",
    "    else:\n",
    "        print(f\"‚úÖ Phase 2 data ready for business insights processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba1765f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:01.999023Z",
     "iopub.status.busy": "2025-11-01T17:47:01.998520Z",
     "iopub.status.idle": "2025-11-01T17:47:02.020304Z",
     "shell.execute_reply": "2025-11-01T17:47:02.019543Z"
    },
    "papermill": {
     "duration": 0.032925,
     "end_time": "2025-11-01T17:47:02.021779",
     "exception": false,
     "start_time": "2025-11-01T17:47:01.988854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üö® STEP 4: GENERATING INVENTORY ALERTS\n",
      "--------------------------------------------------\n",
      "‚úÖ Generated 2 inventory alerts\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 4: Generate Inventory Alerts (Guarded)\n",
    "# -------------------\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping inventory alerts (no uploaded data).\")\n",
    "else:\n",
    "    def generate_inventory_alerts(df):\n",
    "        print(\"\\nüö® STEP 4: GENERATING INVENTORY ALERTS\")\n",
    "        print(\"-\" * 50)\n",
    "        alerts = []\n",
    "        product_summary = df.groupby('product_name').agg({\n",
    "            'sales_qty': ['mean', 'sum'],\n",
    "            'stock_on_hand': 'last',\n",
    "            'category': 'first',\n",
    "            'price': 'mean'\n",
    "        }).reset_index()\n",
    "        product_summary.columns = [\n",
    "            'product_name', 'avg_sales', 'total_sales', \n",
    "            'current_stock', 'category', 'avg_price'\n",
    "        ]\n",
    "        product_summary['weeks_of_stock'] = product_summary['current_stock'] / (product_summary['avg_sales'] + 1e-5)\n",
    "        for _, row in product_summary.iterrows():\n",
    "            urgency, recommended_order, potential_loss = None, 0, 0\n",
    "            if row['weeks_of_stock'] <= 1:\n",
    "                urgency = \"üî¥ CRITICAL\"; recommended_order = int(row['avg_sales'] * 4); potential_loss = int(row['avg_sales'] * row['avg_price'])\n",
    "            elif row['weeks_of_stock'] <= 2:\n",
    "                urgency = \"üü° WARNING\"; recommended_order = int(row['avg_sales'] * 3); potential_loss = int(row['avg_sales'] * row['avg_price'] * 0.5)\n",
    "            elif row['weeks_of_stock'] >= 12:\n",
    "                urgency = \"üîµ OVERSTOCK\"; recommended_order = 0\n",
    "            if urgency is not None:\n",
    "                alerts.append({\n",
    "                    \"Urgency\": urgency,\n",
    "                    \"Product\": row['product_name'],\n",
    "                    \"Category\": row['category'],\n",
    "                    \"Current Stock\": int(row['current_stock']),\n",
    "                    \"Weeks Remaining\": round(row['weeks_of_stock'], 1),\n",
    "                    \"Suggested Order\": recommended_order,\n",
    "                    \"Potential Loss (‚Çπ)\": potential_loss,\n",
    "                    \"Message\": f\"{urgency} - {row['product_name']} has {row['weeks_of_stock']:.1f} weeks of stock left\"\n",
    "                })\n",
    "        print(f\"‚úÖ Generated {len(alerts)} inventory alerts\")\n",
    "        if alerts:\n",
    "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "            alerts_df = pd.DataFrame(alerts)\n",
    "            alerts_df.to_csv(os.path.join(OUTPUT_DIR, \"business_inventory_alerts.csv\"), index=False)\n",
    "        return alerts\n",
    "    if df is not None:\n",
    "        inventory_alerts = generate_inventory_alerts(df)\n",
    "    else:\n",
    "        print(\"‚ùå Data not loaded, skipping inventory alert generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895f1e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:02.043288Z",
     "iopub.status.busy": "2025-11-01T17:47:02.042822Z",
     "iopub.status.idle": "2025-11-01T17:47:02.074026Z",
     "shell.execute_reply": "2025-11-01T17:47:02.073309Z"
    },
    "papermill": {
     "duration": 0.044584,
     "end_time": "2025-11-01T17:47:02.075774",
     "exception": false,
     "start_time": "2025-11-01T17:47:02.031190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 5: DETECTING SALES ANOMALIES\n",
      "-------------------------------------------------------\n",
      "üìä No ML anomaly flags found, using statistical detection...\n",
      "‚úÖ No anomalies detected. Placeholder record created.\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 5: Detect Sales Anomalies (Guarded)\n",
    "# -------------------\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping sales anomalies (no uploaded data).\")\n",
    "else:\n",
    "    def detect_sales_anomalies(df):\n",
    "        print(\"\\nüìä STEP 5: DETECTING SALES ANOMALIES\")\n",
    "        print(\"-\" * 55)\n",
    "        anomalies = []\n",
    "        if 'iso_forest_anomaly' in df.columns:\n",
    "            anomaly_data = df[df['iso_forest_anomaly'] == -1]\n",
    "            print(f\"üîç Found {len(anomaly_data)} flagged anomalies from ML models\")\n",
    "            recent_date = df['week_start'].max() - timedelta(weeks=4)\n",
    "            recent_anomalies = anomaly_data[anomaly_data['week_start'] >= recent_date]\n",
    "            for _, row in recent_anomalies.iterrows():\n",
    "                normal_sales = df[(df['product_name'] == row['product_name']) & (df.get('iso_forest_anomaly', 1) == 1)]['sales_qty'].mean()\n",
    "                if normal_sales > 0:\n",
    "                    change_percent = ((row['sales_qty'] - normal_sales) / normal_sales) * 100\n",
    "                    anomaly_type = \"üìà SALES SPIKE\" if row['sales_qty'] > normal_sales else \"üìâ SALES DROP\"\n",
    "                    reason = \"Promotion/Market factors\" if anomaly_type == \"üìà SALES SPIKE\" else \"Competition/Quality issue\"\n",
    "                    anomalies.append({\n",
    "                        'Type': anomaly_type,\n",
    "                        'Product': row['product_name'],\n",
    "                        'Category': row['category'],\n",
    "                        'Date': row['week_start'].strftime('%Y-%m-%d'),\n",
    "                        'Actual Sales': row['sales_qty'],\n",
    "                        'Normal Sales': round(normal_sales, 1),\n",
    "                        'Change (%)': round(abs(change_percent), 1),\n",
    "                        'Possible Reason': reason,\n",
    "                        'Recommended Action': \"Investigate and adjust inventory planning\"\n",
    "                    })\n",
    "        else:\n",
    "            print(\"üìä No ML anomaly flags found, using statistical detection...\")\n",
    "            for product in df['product_name'].unique():\n",
    "                product_data = df[df['product_name'] == product].sort_values('week_start')\n",
    "                if len(product_data) < 8:\n",
    "                    continue\n",
    "                product_data['sales_ma_4'] = product_data['sales_qty'].rolling(4).mean()\n",
    "                product_data['sales_std_4'] = product_data['sales_qty'].rolling(4).std()\n",
    "                outliers = product_data[abs(product_data['sales_qty'] - product_data['sales_ma_4']) > 2 * product_data['sales_std_4']]\n",
    "                for _, row in outliers.tail(2).iterrows():\n",
    "                    normal_sales = row['sales_ma_4']\n",
    "                    if pd.notna(normal_sales) and normal_sales > 0:\n",
    "                        change_percent = ((row['sales_qty'] - normal_sales) / normal_sales) * 100\n",
    "                        anomaly_type = \"üìà SALES SPIKE\" if row['sales_qty'] > normal_sales else \"üìâ SALES DROP\"\n",
    "                        anomalies.append({\n",
    "                            'Type': anomaly_type,\n",
    "                            'Product': product,\n",
    "                            'Category': row['category'],\n",
    "                            'Date': row['week_start'].strftime('%Y-%m-%d'),\n",
    "                            'Actual Sales': row['sales_qty'],\n",
    "                            'Normal Sales': round(normal_sales, 1),\n",
    "                            'Change (%)': round(abs(change_percent), 1),\n",
    "                            'Possible Reason': \"Statistical anomaly detected\",\n",
    "                            'Recommended Action': \"Review and investigate cause\"\n",
    "                        })\n",
    "        if not anomalies:\n",
    "            anomalies.append({\n",
    "                'Type': \"NONE\",\n",
    "                'Product': \"N/A\",\n",
    "                'Category': \"N/A\",\n",
    "                'Date': df['week_start'].max().strftime('%Y-%m-%d') if 'week_start' in df.columns else \"N/A\",\n",
    "                'Actual Sales': 0,\n",
    "                'Normal Sales': 0,\n",
    "                'Change (%)': 0,\n",
    "                'Possible Reason': \"No anomalies detected\",\n",
    "                'Recommended Action': \"No action required\"\n",
    "            })\n",
    "            print(\"‚úÖ No anomalies detected. Placeholder record created.\")\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        pd.DataFrame(anomalies).to_csv(os.path.join(OUTPUT_DIR, \"business_sales_anomalies.csv\"), index=False)\n",
    "        return anomalies\n",
    "    if df is not None:\n",
    "        sales_anomalies = detect_sales_anomalies(df)\n",
    "    else:\n",
    "        print(\"‚ùå Data not loaded, skipping sales anomaly detection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f72e13a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:02.095535Z",
     "iopub.status.busy": "2025-11-01T17:47:02.095164Z",
     "iopub.status.idle": "2025-11-01T17:47:02.111496Z",
     "shell.execute_reply": "2025-11-01T17:47:02.110767Z"
    },
    "papermill": {
     "duration": 0.027529,
     "end_time": "2025-11-01T17:47:02.112919",
     "exception": false,
     "start_time": "2025-11-01T17:47:02.085390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ STEP 6: SEASONAL DEMAND ANALYSIS\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 6: Seasonal Demand Analysis (Guarded)\n",
    "# -------------------\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping seasonal demand analysis (no uploaded data).\")\n",
    "else:\n",
    "    def analyze_seasonal_patterns(df):\n",
    "        print(\"\\nüìÖ STEP 6: SEASONAL DEMAND ANALYSIS\")\n",
    "        print(\"-\" * 55)\n",
    "        seasonal_insights = []\n",
    "        if \"season\" not in df.columns:\n",
    "            df[\"month\"] = df[\"week_start\"].dt.month\n",
    "            df[\"season\"] = df[\"month\"].map({\n",
    "                12: \"Winter\", 1: \"Winter\", 2: \"Winter\",\n",
    "                3: \"Spring\", 4: \"Spring\", 5: \"Spring\",\n",
    "                6: \"Summer\", 7: \"Summer\", 8: \"Summer\",\n",
    "                9: \"Fall\", 10: \"Fall\", 11: \"Fall\"\n",
    "            })\n",
    "        category_seasonal = df.groupby([\"category\", \"season\"]) [\"sales_qty\"].mean().reset_index()\n",
    "        for category in df[\"category\"].unique():\n",
    "            cat_data = category_seasonal[category_seasonal[\"category\"] == category]\n",
    "            if len(cat_data) > 1:\n",
    "                peak = cat_data.loc[cat_data[\"sales_qty\"].idxmax()]\n",
    "                low = cat_data.loc[cat_data[\"sales_qty\"].idxmin()]\n",
    "                if low[\"sales_qty\"] > 0:\n",
    "                    variation = ((peak[\"sales_qty\"] - low[\"sales_qty\"]) / low[\"sales_qty\"]) * 100\n",
    "                    if variation > 50:\n",
    "                        recommendation = f\"üöÄ High seasonality! Increase {category} stock by 40‚Äì60% before {peak['season']}\"\n",
    "                        action = \"Plan aggressive promotions and inventory buildup\"\n",
    "                    elif variation > 25:\n",
    "                        recommendation = f\"üìà Moderate seasonality. Plan 25‚Äì40% inventory boost for {peak['season']}\"\n",
    "                        action = \"Adjust supply chain capacity ahead of demand spike\"\n",
    "                    else:\n",
    "                        recommendation = f\"‚úÖ Low seasonality. Maintain steady inventory levels\"\n",
    "                        action = \"No major changes required\"\n",
    "                    seasonal_insights.append({\n",
    "                        \"Category\": category,\n",
    "                        \"Peak Season\": peak[\"season\"],\n",
    "                        \"Peak Avg Sales\": round(peak[\"sales_qty\"], 1),\n",
    "                        \"Low Season\": low[\"season\"],\n",
    "                        \"Low Avg Sales\": round(low[\"sales_qty\"], 1),\n",
    "                        \"Variation (%)\": round(variation, 1),\n",
    "                        \"Recommendation\": recommendation,\n",
    "                        \"Business Action\": action\n",
    "                    })\n",
    "        if not seasonal_insights:\n",
    "            seasonal_insights.append({\n",
    "                \"Category\": \"N/A\",\n",
    "                \"Peak Season\": \"N/A\",\n",
    "                \"Peak Avg Sales\": 0,\n",
    "                \"Low Season\": \"N/A\",\n",
    "                \"Low Avg Sales\": 0,\n",
    "                \"Variation (%)\": 0,\n",
    "                \"Recommendation\": \"No seasonal patterns detected\",\n",
    "                \"Business Action\": \"No action required\"\n",
    "            })\n",
    "            print(\"‚úÖ No strong seasonal patterns found. Placeholder record created.\")\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        pd.DataFrame(seasonal_insights).to_csv(os.path.join(OUTPUT_DIR, \"business_seasonal_insights.csv\"), index=False)\n",
    "        return seasonal_insights\n",
    "    if df is not None:\n",
    "        seasonal_insights = analyze_seasonal_patterns(df)\n",
    "    else:\n",
    "        print(\"‚ùå Data not loaded, skipping seasonal demand analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863f34ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:02.134306Z",
     "iopub.status.busy": "2025-11-01T17:47:02.133936Z",
     "iopub.status.idle": "2025-11-01T17:47:02.151003Z",
     "shell.execute_reply": "2025-11-01T17:47:02.150175Z"
    },
    "papermill": {
     "duration": 0.02885,
     "end_time": "2025-11-01T17:47:02.152354",
     "exception": false,
     "start_time": "2025-11-01T17:47:02.123504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ STEP 7: PRICE OPTIMIZATION ANALYSIS\n",
      "-------------------------------------------------------\n",
      "‚úÖ Found 5 pricing opportunities\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 7: Price Optimization Analysis (Guarded)\n",
    "# -------------------\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping price optimization analysis (no uploaded data).\")\n",
    "else:\n",
    "    def analyze_price_optimization(df):\n",
    "        print(\"\\nüí∞ STEP 7: PRICE OPTIMIZATION ANALYSIS\")\n",
    "        print(\"-\" * 55)\n",
    "        price_insights = []\n",
    "        for product in df[\"product_name\"].unique():\n",
    "            product_data = df[df[\"product_name\"] == product].sort_values(\"week_start\")\n",
    "            if len(product_data) < 4:\n",
    "                continue\n",
    "            if product_data[\"price\"].std() > 0:\n",
    "                correlation = product_data[\"price\"].corr(product_data[\"sales_qty\"])\n",
    "                current_price = product_data[\"price\"].iloc[-1]\n",
    "                avg_sales = product_data[\"sales_qty\"].mean()\n",
    "                if correlation < -0.3:\n",
    "                    insight_type = \"üîª PRICE SENSITIVE\"; action = \"üí° Test 5‚Äì10% price reduction\"; recommendation = \"Lower prices can significantly boost sales\"\n",
    "                elif correlation > 0.3:\n",
    "                    insight_type = \"‚¨ÜÔ∏è PREMIUM OPPORTUNITY\"; action = \"üí° Test 5‚Äì15% price increase\"; recommendation = \"Higher prices may signal quality and increase demand\"\n",
    "                elif abs(correlation) < 0.1:\n",
    "                    insight_type = \"üí° PRICE INELASTIC\"; action = \"‚öñÔ∏è Focus on cost optimization instead of price changes\"; recommendation = \"Price changes have little effect on demand\"\n",
    "                else:\n",
    "                    continue\n",
    "                price_insights.append({\n",
    "                    \"Product\": product,\n",
    "                    \"Category\": product_data[\"category\"].iloc[0],\n",
    "                    \"Current Price (‚Çπ)\": round(current_price, 2),\n",
    "                    \"Average Weekly Sales\": round(avg_sales, 1),\n",
    "                    \"Price-Sales Correlation\": round(correlation, 2),\n",
    "                    \"Insight Type\": insight_type,\n",
    "                    \"Recommendation\": recommendation,\n",
    "                    \"Suggested Action\": action,\n",
    "                    \"Priority Score\": round(avg_sales * abs(correlation), 2)\n",
    "                })\n",
    "        price_insights = sorted(price_insights, key=lambda x: x[\"Priority Score\"], reverse=True)\n",
    "        print(f\"‚úÖ Found {len(price_insights)} pricing opportunities\")\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        pd.DataFrame(price_insights).to_csv(os.path.join(OUTPUT_DIR, \"business_pricing_opportunities.csv\"), index=False)\n",
    "        return price_insights\n",
    "    if df is not None:\n",
    "        price_insights = analyze_price_optimization(df)\n",
    "    else:\n",
    "        print(\"‚ùå Data not loaded, skipping price optimization analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45fba3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:02.175926Z",
     "iopub.status.busy": "2025-11-01T17:47:02.175651Z",
     "iopub.status.idle": "2025-11-01T17:47:03.577728Z",
     "shell.execute_reply": "2025-11-01T17:47:03.576975Z"
    },
    "papermill": {
     "duration": 1.41625,
     "end_time": "2025-11-01T17:47:03.579162",
     "exception": false,
     "start_time": "2025-11-01T17:47:02.162912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 8: EXECUTIVE DASHBOARD\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Executive dashboard saved as: F:\\RetailSense_Lite\\outputs\\executive_dashboard.png\n",
      "\n",
      "üìà EXECUTIVE SUMMARY\n",
      "üõçÔ∏è  Total Products: 5\n",
      "üì¶ Total Sales Volume: 108,453.20000000001 units\n",
      "üí∞ Total Revenue: ‚Çπ18,532,638.68\n",
      "üíµ Average Price: ‚Çπ171.13\n",
      "\n",
      "üö® Inventory Alerts:\n",
      "   ‚Ä¢ Critical: 0\n",
      "   ‚Ä¢ Warning: 2\n",
      "   ‚Ä¢ Overstock: 0\n",
      "\n",
      "üìä Key Insights:\n",
      "   ‚Ä¢ Sales Anomalies Detected: 1\n",
      "   ‚Ä¢ Seasonal Patterns Analyzed: 5 categories\n",
      "   ‚Ä¢ Price Optimization Opportunities: 5\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 8: Executive Dashboard (Guarded)\n",
    "# -------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping executive dashboard (no uploaded data).\")\n",
    "else:\n",
    "    def create_executive_dashboard(df, alerts, anomalies, seasonal, pricing):\n",
    "        print(\"\\nüìä STEP 8: EXECUTIVE DASHBOARD\")\n",
    "        print(\"-\" * 55)\n",
    "        total_products = df['product_name'].nunique()\n",
    "        total_sales = df['sales_qty'].sum()\n",
    "        total_revenue = (df['sales_qty'] * df['price']).sum()\n",
    "        avg_price = df['price'].mean()\n",
    "        critical_alerts = len([a for a in alerts if 'üî¥' in a['Urgency']]) if alerts else 0\n",
    "        warning_alerts = len([a for a in alerts if 'üü°' in a['Urgency']]) if alerts else 0\n",
    "        overstock_alerts = len([a for a in alerts if 'üîµ' in a['Urgency']]) if alerts else 0\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('RetailSense Executive Dashboard', fontsize=16, fontweight='bold')\n",
    "        alert_labels = ['Critical', 'Warning', 'Overstock']\n",
    "        alert_counts = [critical_alerts, warning_alerts, overstock_alerts]\n",
    "        axes[0, 0].pie(alert_counts, labels=alert_labels, autopct='%1.1f%%', startangle=90, \n",
    "                       colors=['#FF4C4C','#FFC300','#4C9AFF'])\n",
    "        axes[0, 0].set_title('Inventory Alert Distribution')\n",
    "        category_sales = df.groupby('category')['sales_qty'].sum().sort_values(ascending=False).head(8)\n",
    "        axes[0, 1].bar(category_sales.index, category_sales.values, color='#4CAF50', alpha=0.7)\n",
    "        axes[0, 1].set_title('Top Categories by Sales Volume')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        weekly_sales = df.groupby('week_start')['sales_qty'].sum()\n",
    "        axes[1, 0].plot(weekly_sales.index, weekly_sales.values, marker='o', linestyle='-', color='#FF5733')\n",
    "        axes[1, 0].set_title('Sales Trend Over Time')\n",
    "        axes[1, 0].set_xlabel('Week')\n",
    "        axes[1, 0].set_ylabel('Total Sales')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        sample_products = (\n",
    "            df.groupby('product_name')\n",
    "              .agg({'price':'mean', 'sales_qty':'sum'})\n",
    "              .reset_index()\n",
    "              .sample(min(50, len(df['product_name'].unique())))\n",
    "        )\n",
    "        axes[1, 1].scatter(sample_products['price'], sample_products['sales_qty'], alpha=0.6, color='#FFAA00')\n",
    "        axes[1, 1].set_title('Price vs Total Sales (Sample Products)')\n",
    "        axes[1, 1].set_xlabel('Average Price (‚Çπ)')\n",
    "        axes[1, 1].set_ylabel('Total Sales')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        dashboard_image_path = os.path.join(OUTPUT_DIR, 'executive_dashboard.png')\n",
    "        plt.savefig(dashboard_image_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"‚úÖ Executive dashboard saved as: {dashboard_image_path}\")\n",
    "        print(\"\\nüìà EXECUTIVE SUMMARY\")\n",
    "        print(f\"üõçÔ∏è  Total Products: {total_products}\")\n",
    "        print(f\"üì¶ Total Sales Volume: {total_sales:,} units\")\n",
    "        print(f\"üí∞ Total Revenue: ‚Çπ{total_revenue:,.2f}\")\n",
    "        print(f\"üíµ Average Price: ‚Çπ{avg_price:.2f}\")\n",
    "        print(\"\\nüö® Inventory Alerts:\")\n",
    "        print(f\"   ‚Ä¢ Critical: {critical_alerts}\")\n",
    "        print(f\"   ‚Ä¢ Warning: {warning_alerts}\")\n",
    "        print(f\"   ‚Ä¢ Overstock: {overstock_alerts}\")\n",
    "        print(\"\\nüìä Key Insights:\")\n",
    "        print(f\"   ‚Ä¢ Sales Anomalies Detected: {len(anomalies) if anomalies else 0}\")\n",
    "        print(f\"   ‚Ä¢ Seasonal Patterns Analyzed: {len(seasonal) if seasonal else 0} categories\")\n",
    "        print(f\"   ‚Ä¢ Price Optimization Opportunities: {len(pricing) if pricing else 0}\")\n",
    "    if df is not None:\n",
    "        create_executive_dashboard(df, inventory_alerts, sales_anomalies, seasonal_insights, price_insights)\n",
    "    else:\n",
    "        print(\"‚ùå Data not loaded, skipping executive dashboard creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979d4c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:03.611412Z",
     "iopub.status.busy": "2025-11-01T17:47:03.611109Z",
     "iopub.status.idle": "2025-11-01T17:47:03.628059Z",
     "shell.execute_reply": "2025-11-01T17:47:03.627203Z"
    },
    "papermill": {
     "duration": 0.035095,
     "end_time": "2025-11-01T17:47:03.629580",
     "exception": false,
     "start_time": "2025-11-01T17:47:03.594485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ STEP 9: SAVING BUSINESS INSIGHTS\n",
      "-------------------------------------------------------\n",
      "‚úÖ Saved: F:\\RetailSense_Lite\\outputs\\business_inventory_alerts.csv\n",
      "‚úÖ Saved: F:\\RetailSense_Lite\\outputs\\business_sales_anomalies.csv\n",
      "‚úÖ Saved: F:\\RetailSense_Lite\\outputs\\business_seasonal_insights.csv\n",
      "‚úÖ Saved: F:\\RetailSense_Lite\\outputs\\business_pricing_opportunities.csv\n",
      "‚úÖ Saved: F:\\RetailSense_Lite\\outputs\\phase3_completion_report.csv\n",
      "\n",
      "üéâ ALL INSIGHTS SAVED SUCCESSFULLY IN: F:\\RetailSense_Lite\\outputs\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 9: Save Business Insights (Phase 3 Completion) (Guarded)\n",
    "# -------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping save step (no uploaded data).\")\n",
    "else:\n",
    "    def save_all_insights(alerts, anomalies, seasonal, pricing, output_dir=OUTPUT_DIR):\n",
    "        print(\"\\nüíæ STEP 9: SAVING BUSINESS INSIGHTS\")\n",
    "        print(\"-\" * 55)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        try:\n",
    "            if alerts:\n",
    "                inventory_path = os.path.join(output_dir, \"business_inventory_alerts.csv\")\n",
    "                pd.DataFrame(alerts).to_csv(inventory_path, index=False)\n",
    "                print(f\"‚úÖ Saved: {inventory_path}\")\n",
    "            anomalies_path = os.path.join(output_dir, \"business_sales_anomalies.csv\")\n",
    "            pd.DataFrame(anomalies or []).to_csv(anomalies_path, index=False)\n",
    "            print(f\"‚úÖ Saved: {anomalies_path}\")\n",
    "            if seasonal:\n",
    "                seasonal_path = os.path.join(output_dir, \"business_seasonal_insights.csv\")\n",
    "                pd.DataFrame(seasonal).to_csv(seasonal_path, index=False)\n",
    "                print(f\"‚úÖ Saved: {seasonal_path}\")\n",
    "            if pricing:\n",
    "                pricing_path = os.path.join(output_dir, \"business_pricing_opportunities.csv\")\n",
    "                pd.DataFrame(pricing).to_csv(pricing_path, index=False)\n",
    "                print(f\"‚úÖ Saved: {pricing_path}\")\n",
    "            summary_report = {\n",
    "                \"report_generated\": datetime.now().isoformat(),\n",
    "                \"total_inventory_alerts\": len(alerts or []),\n",
    "                \"total_sales_anomalies\": len(anomalies or []),\n",
    "                \"seasonal_insights_generated\": len(seasonal or []),\n",
    "                \"pricing_opportunities\": len(pricing or []),\n",
    "                \"phase_3_complete\": True,\n",
    "                \"ready_for_dashboard\": True\n",
    "            }\n",
    "            report_path = os.path.join(output_dir, \"phase3_completion_report.csv\")\n",
    "            pd.DataFrame([summary_report]).to_csv(report_path, index=False)\n",
    "            print(f\"‚úÖ Saved: {report_path}\")\n",
    "            print(f\"\\nüéâ ALL INSIGHTS SAVED SUCCESSFULLY IN: {output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error while saving insights: {e}\")\n",
    "    if df is not None:\n",
    "        save_all_insights(inventory_alerts, sales_anomalies, seasonal_insights, price_insights)\n",
    "    else:\n",
    "        print(\"‚ùå Data not loaded, skipping save step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b27ea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:47:03.655685Z",
     "iopub.status.busy": "2025-11-01T17:47:03.655389Z",
     "iopub.status.idle": "2025-11-01T17:47:03.663882Z",
     "shell.execute_reply": "2025-11-01T17:47:03.663131Z"
    },
    "papermill": {
     "duration": 0.023994,
     "end_time": "2025-11-01T17:47:03.665768",
     "exception": false,
     "start_time": "2025-11-01T17:47:03.641774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ PHASE 3: BUSINESS LAYER & INSIGHTS COMPLETED!\n",
      "======================================================================\n",
      "üìÖ Completed: 2025-11-01 23:17:03\n",
      "\n",
      "‚úÖ DELIVERABLES GENERATED (saved in 'F:\\RetailSense_Lite\\outputs'):\n",
      "   üìä Executive Dashboard: F:\\RetailSense_Lite\\outputs\\executive_dashboard.png\n",
      "   üìã Inventory Alerts: F:\\RetailSense_Lite\\outputs\\business_inventory_alerts.csv\n",
      "   üö® Sales Anomalies: F:\\RetailSense_Lite\\outputs\\business_sales_anomalies.csv\n",
      "   üìÖ Seasonal Insights: F:\\RetailSense_Lite\\outputs\\business_seasonal_insights.csv\n",
      "   üí∞ Pricing Opportunities: F:\\RetailSense_Lite\\outputs\\business_pricing_opportunities.csv\n",
      "   üìÑ Completion Report: F:\\RetailSense_Lite\\outputs\\phase3_completion_report.csv\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   ‚Üí Phase 4: Build interactive Streamlit dashboard\n",
      "   ‚Üí Phase 5: Deploy to cloud platform\n",
      "   ‚Üí Phase 6: Add innovation features\n",
      "\n",
      "üí° KEY INSIGHTS GENERATED:\n",
      "   ‚Ä¢ 2 inventory recommendations\n",
      "   ‚Ä¢ 1 anomalies investigated\n",
      "   ‚Ä¢ 5 seasonal patterns identified\n",
      "   ‚Ä¢ 5 pricing opportunities found\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 10: Phase 3 Completion Summary (Guarded)\n",
    "# -------------------\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "if 'SKIP_PHASE3' in globals() and SKIP_PHASE3:\n",
    "    print(\"‚è≠Ô∏è Skipping Phase 3 summary (no uploaded data).\")\n",
    "else:\n",
    "    def phase3_completion_summary(df, alerts, anomalies, seasonal, pricing, output_dir=OUTPUT_DIR):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üéØ PHASE 3: BUSINESS LAYER & INSIGHTS COMPLETED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"üìÖ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        if df is not None:\n",
    "            print(f\"\\n‚úÖ DELIVERABLES GENERATED (saved in '{output_dir}'):\")\n",
    "            print(f\"   üìä Executive Dashboard: {os.path.join(output_dir, 'executive_dashboard.png')}\")\n",
    "            print(f\"   üìã Inventory Alerts: {os.path.join(output_dir, 'business_inventory_alerts.csv')}\")\n",
    "            print(f\"   üö® Sales Anomalies: {os.path.join(output_dir, 'business_sales_anomalies.csv')}\")\n",
    "            print(f\"   üìÖ Seasonal Insights: {os.path.join(output_dir, 'business_seasonal_insights.csv')}\")\n",
    "            print(f\"   üí∞ Pricing Opportunities: {os.path.join(output_dir, 'business_pricing_opportunities.csv')}\")\n",
    "            print(f\"   üìÑ Completion Report: {os.path.join(output_dir, 'phase3_completion_report.csv')}\")\n",
    "            print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "            print(f\"   ‚Üí Phase 4: Build interactive Streamlit dashboard\")\n",
    "            print(f\"   ‚Üí Phase 5: Deploy to cloud platform\")\n",
    "            print(f\"   ‚Üí Phase 6: Add innovation features\")\n",
    "            print(f\"\\nüí° KEY INSIGHTS GENERATED:\")\n",
    "            print(f\"   ‚Ä¢ {len(alerts or [])} inventory recommendations\")\n",
    "            if anomalies and len(anomalies) == 1 and isinstance(anomalies[0], dict) and anomalies[0].get(\"type\") == \"NONE\":\n",
    "                print(f\"   ‚Ä¢ No anomalies detected\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {len(anomalies or [])} anomalies investigated\")\n",
    "            print(f\"   ‚Ä¢ {len(seasonal or [])} seasonal patterns identified\")\n",
    "            print(f\"   ‚Ä¢ {len(pricing or [])} pricing opportunities found\")\n",
    "        else:\n",
    "            print(\"‚ùå Phase 3 could not complete due to data loading issues\")\n",
    "            print(\"üí° Please ensure Phase 2 was completed successfully\")\n",
    "        print(\"=\"*70)\n",
    "    if df is not None:\n",
    "        phase3_completion_summary(df, inventory_alerts, sales_anomalies, seasonal_insights, price_insights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retailsense_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.371844,
   "end_time": "2025-11-01T17:47:04.899154",
   "environment_variables": {},
   "exception": null,
   "input_path": "F:\\RetailSense_Lite\\notebooks\\phase3.ipynb",
   "output_path": "F:\\RetailSense_Lite\\notebooks\\phase3_out.ipynb",
   "parameters": {},
   "start_time": "2025-11-01T17:46:55.527310",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}