{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb73213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:37.194790Z",
     "iopub.status.busy": "2025-11-01T17:46:37.194405Z",
     "iopub.status.idle": "2025-11-01T17:46:37.233158Z",
     "shell.execute_reply": "2025-11-01T17:46:37.232122Z"
    },
    "papermill": {
     "duration": 0.052274,
     "end_time": "2025-11-01T17:46:37.235690",
     "exception": false,
     "start_time": "2025-11-01T17:46:37.183416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ BASE_DIR: F:\\RetailSense_Lite\n",
      "üìÅ PROCESSED_DIR: F:\\RetailSense_Lite\\data\\processed\n",
      "üìÅ OUTPUT_DIR: F:\\RetailSense_Lite\\outputs\n",
      "üìÑ CLEANED_DATA_PATH: F:\\RetailSense_Lite\\data\\processed\\cleaned_data.csv\n",
      "üìÑ FEATURES_DATA_PATH: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS (Papermill-friendly)\n",
    "# Configure base directory and derive common paths for Phase 2\n",
    "import os\n",
    "\n",
    "# Prefer env var for robustness when papermill parameters cell tag is missing\n",
    "_env_base = os.getenv(\"RETAILSENSE_BASE_DIR\")\n",
    "if _env_base and _env_base.strip():\n",
    "    RETAILSENSE_BASE_DIR = _env_base\n",
    "elif 'RETAILSENSE_BASE_DIR' not in globals() or not RETAILSENSE_BASE_DIR:\n",
    "    RETAILSENSE_BASE_DIR = r\"F:\\RetailSense_Lite\"\n",
    "\n",
    "BASE_DIR = RETAILSENSE_BASE_DIR\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'outputs')\n",
    "NOTEBOOKS_DIR = os.path.join(BASE_DIR, 'notebooks')\n",
    "\n",
    "CLEANED_DATA_PATH = os.path.join(PROCESSED_DIR, 'cleaned_data.csv')\n",
    "FEATURES_DATA_PATH = os.path.join(PROCESSED_DIR, 'data_with_all_features.csv')\n",
    "FORECAST_CSV = os.path.join(OUTPUT_DIR, 'forecasting_results.csv')\n",
    "ANOMALIES_CSV = os.path.join(OUTPUT_DIR, 'anomalies.csv')\n",
    "\n",
    "# Ensure all paths are absolute\n",
    "FEATURES_DATA_PATH = os.path.abspath(FEATURES_DATA_PATH)\n",
    "CLEANED_DATA_PATH = os.path.abspath(CLEANED_DATA_PATH)\n",
    "FORECAST_CSV = os.path.abspath(FORECAST_CSV)\n",
    "ANOMALIES_CSV = os.path.abspath(ANOMALIES_CSV)\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"üìÅ PROCESSED_DIR: {PROCESSED_DIR}\")\n",
    "print(f\"üìÅ OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"üìÑ CLEANED_DATA_PATH: {CLEANED_DATA_PATH}\")\n",
    "print(f\"üìÑ FEATURES_DATA_PATH: {FEATURES_DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f28ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:37.260099Z",
     "iopub.status.busy": "2025-11-01T17:46:37.259673Z",
     "iopub.status.idle": "2025-11-01T17:46:41.527651Z",
     "shell.execute_reply": "2025-11-01T17:46:41.526820Z"
    },
    "papermill": {
     "duration": 4.280184,
     "end_time": "2025-11-01T17:46:41.529116",
     "exception": false,
     "start_time": "2025-11-01T17:46:37.248932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current Working Directory: F:\\RetailSense_Lite\n",
      "üîß Project Root Directory: F:\\RetailSense_Lite\n",
      "üìÅ Output Directory: F:\\RetailSense_Lite\\outputs\n",
      "üìö Input (Cleaned Data) Path: F:\\RetailSense_Lite\\data\\processed\\cleaned_data.csv\n",
      "üìö Output (Feature Data) Path: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "üìö Forecast CSV Path: F:\\RetailSense_Lite\\outputs\\forecasting_results.csv\n",
      "üìö Anomalies CSV Path: F:\\RetailSense_Lite\\outputs\\anomalies.csv\n",
      "üîç sys.path includes: ['f:\\\\RetailSense_Lite\\\\retailsense_env', 'f:\\\\RetailSense_Lite\\\\retailsense_env\\\\Lib\\\\site-packages', 'f:\\\\RetailSense_Lite\\\\retailsense_env\\\\Lib\\\\site-packages\\\\win32', 'f:\\\\RetailSense_Lite\\\\retailsense_env\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'f:\\\\RetailSense_Lite\\\\retailsense_env\\\\Lib\\\\site-packages\\\\Pythonwin', 'F:\\\\RetailSense_Lite']\n",
      "\n",
      "üöÄ RETAILSENSE PHASE 2: FEATURE ENGINEERING & MODELS INITIATED\n",
      "============================================================\n",
      "üìÖ Started at: 2025-11-01 23:16:41\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports for Phase 2\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------\n",
    "# Use parameterized base/paths from Parameters cell\n",
    "# --------------------------\n",
    "PROJECT_ROOT = BASE_DIR\n",
    "DATA_CLEANED_PATH = CLEANED_DATA_PATH\n",
    "DATA_FEATURES_PATH = FEATURES_DATA_PATH\n",
    "OUTPUT_DIR = OUTPUT_DIR\n",
    "FORECAST_CSV = FORECAST_CSV\n",
    "ANOMALIES_CSV = ANOMALIES_CSV\n",
    "NOTEBOOKS_DIR = NOTEBOOKS_DIR\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Fix sys.path to see project root and models folder\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# --------------------------\n",
    "# Import custom modules\n",
    "# --------------------------\n",
    "from models.feature_engineering import FeatureEngineering\n",
    "from models.baselines import BaselineModels\n",
    "from models.forecasting import AdvancedForecasting\n",
    "from models.anomaly_detection import AnomalyDetection\n",
    "\n",
    "# --------------------------\n",
    "# Debugging Info\n",
    "# --------------------------\n",
    "print(f\"üìÇ Current Working Directory: {os.getcwd()}\")\n",
    "print(f\"üîß Project Root Directory: {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"üìö Input (Cleaned Data) Path: {DATA_CLEANED_PATH}\")\n",
    "print(f\"üìö Output (Feature Data) Path: {DATA_FEATURES_PATH}\")\n",
    "print(f\"üìö Forecast CSV Path: {FORECAST_CSV}\")\n",
    "print(f\"üìö Anomalies CSV Path: {ANOMALIES_CSV}\")\n",
    "print(f\"üîç sys.path includes: {[p for p in sys.path if 'RetailSense_Lite' in p]}\")\n",
    "\n",
    "print(\"\\nüöÄ RETAILSENSE PHASE 2: FEATURE ENGINEERING & MODELS INITIATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c32e7e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:41.549278Z",
     "iopub.status.busy": "2025-11-01T17:46:41.548733Z",
     "iopub.status.idle": "2025-11-01T17:46:41.570702Z",
     "shell.execute_reply": "2025-11-01T17:46:41.569962Z"
    },
    "papermill": {
     "duration": 0.03376,
     "end_time": "2025-11-01T17:46:41.572113",
     "exception": false,
     "start_time": "2025-11-01T17:46:41.538353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Project root set to: F:\\RetailSense_Lite\n",
      "üîç Models folder present? True\n",
      "‚úÖ FeatureEngineering initialized\n",
      "‚úÖ Phase 2 components initialized successfully\n",
      "üßπ Cleared previous Phase 2 outputs to prevent stale results.\n",
      "‚úÖ Using uploaded data file: F:\\RetailSense_Lite\\data\\uploaded\\uploaded_data.csv\n",
      "üì• Uploaded dataset loaded: 525 rows √ó 16 columns\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize Phase 2 Components + Upload Gate\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# =====================================================\n",
    "# Setup Project Environment (parameterized)\n",
    "# =====================================================\n",
    "PROJECT_ROOT = BASE_DIR\n",
    "\n",
    "# Ensure sys.path includes the project root for module imports\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"üìÇ Project root set to: {PROJECT_ROOT}\")\n",
    "print(f\"üîç Models folder present? {'models' in os.listdir(PROJECT_ROOT)}\")\n",
    "\n",
    "# =====================================================\n",
    "# Utility: Clear all Phase 2 outputs to avoid stale dashboards\n",
    "# =====================================================\n",
    "def clear_phase2_outputs():\n",
    "    try:\n",
    "        # Known output files to remove\n",
    "        paths = [\n",
    "            os.path.join(OUTPUT_DIR, \"model_performance_comparison.png\"),\n",
    "            os.path.join(OUTPUT_DIR, \"phase2_business_insights.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"forecasting_results.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"anomalies.csv\"),\n",
    "            os.path.join(OUTPUT_DIR, \"xgboost_model.pkl\"),\n",
    "            os.path.join(OUTPUT_DIR, \"lightgbm_model.pkl\"),\n",
    "            os.path.join(OUTPUT_DIR, \"isolation_forest_model.pkl\"),\n",
    "            os.path.join(OUTPUT_DIR, \"ocsvm_model.pkl\"),\n",
    "            os.path.join(OUTPUT_DIR, \"anomaly_dashboard.png\")\n",
    "        ]\n",
    "        for p in paths:\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "        # Also clear engineered dataset to avoid accidental reuse\n",
    "        if os.path.exists(FEATURES_DATA_PATH):\n",
    "            os.remove(FEATURES_DATA_PATH)\n",
    "        print(\"üßπ Cleared previous Phase 2 outputs to prevent stale results.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to clear some outputs: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# Import Custom Modules\n",
    "# =====================================================\n",
    "from models.feature_engineering import FeatureEngineering\n",
    "from models.baselines import BaselineModels\n",
    "from models.forecasting import AdvancedForecasting\n",
    "from models.anomaly_detection import AnomalyDetection\n",
    "\n",
    "# =====================================================\n",
    "# Initialize Phase 2 Components\n",
    "# =====================================================\n",
    "def initialize_phase2_components():\n",
    "    \"\"\"Initialize all Phase 2 ML pipeline components.\"\"\"\n",
    "    components = {\n",
    "        \"feature_engineering\": FeatureEngineering(),     # Feature creation\n",
    "        \"baseline_models\": BaselineModels(),             # ARIMA & Prophet\n",
    "        \"advanced_forecasting\": AdvancedForecasting(),   # XGBoost & LightGBM\n",
    "        \"anomaly_detection\": AnomalyDetection()          # Isolation Forest & One-Class SVM\n",
    "    }\n",
    "    print(\"‚úÖ Phase 2 components initialized successfully\")\n",
    "    return components\n",
    "\n",
    "# Instantiate pipeline components\n",
    "components = initialize_phase2_components()\n",
    "\n",
    "# =====================================================\n",
    "# Require Uploaded Data Gate\n",
    "# =====================================================\n",
    "# Priority: env var UPLOADED_DATA_PATH -> default path under data/uploaded/\n",
    "DEFAULT_UPLOAD_PATH = os.path.join(BASE_DIR, 'data', 'uploaded', 'uploaded_data.csv')\n",
    "UPLOADED_DATA_PATH = os.getenv('UPLOADED_DATA_PATH')\n",
    "if not UPLOADED_DATA_PATH or not UPLOADED_DATA_PATH.strip():\n",
    "    UPLOADED_DATA_PATH = DEFAULT_UPLOAD_PATH\n",
    "\n",
    "# Normalize path\n",
    "UPLOADED_DATA_PATH = os.path.normpath(UPLOADED_DATA_PATH)\n",
    "\n",
    "SKIP_PHASE2 = False\n",
    "if not os.path.exists(UPLOADED_DATA_PATH):\n",
    "    # No uploaded file -> clear outputs and skip downstream execution\n",
    "    clear_phase2_outputs()\n",
    "    SKIP_PHASE2 = True\n",
    "    print(\"‚ùå No uploaded data file found.\")\n",
    "    print(\"   Set env 'UPLOADED_DATA_PATH' or place file at: \" + DEFAULT_UPLOAD_PATH)\n",
    "    print(\"   Phase 2 execution will be skipped to avoid showing stale results.\")\n",
    "else:\n",
    "    # Use the uploaded file as the canonical cleaned input for Phase 2\n",
    "    CLEANED_DATA_PATH = UPLOADED_DATA_PATH\n",
    "    # Clear outputs before generating new ones to ensure dashboard reflects only this upload\n",
    "    clear_phase2_outputs()\n",
    "    print(f\"‚úÖ Using uploaded data file: {CLEANED_DATA_PATH}\")\n",
    "\n",
    "# =====================================================\n",
    "# Load Cleaned Data from Uploaded file (optionally filter by selected product)\n",
    "# =====================================================\n",
    "if not SKIP_PHASE2:\n",
    "    DATA_CLEANED_PATH = CLEANED_DATA_PATH\n",
    "\n",
    "    if os.path.exists(DATA_CLEANED_PATH):\n",
    "        df_cleaned = pd.read_csv(DATA_CLEANED_PATH)\n",
    "        print(f\"üì• Uploaded dataset loaded: {df_cleaned.shape[0]} rows √ó {df_cleaned.shape[1]} columns\")\n",
    "\n",
    "        # Optional filtering: PRODUCT_NAME may actually be an ID or name; support common columns\n",
    "        try:\n",
    "            selected_value = None\n",
    "            # Prefer environment variable for robustness\n",
    "            env_product = os.getenv(\"PRODUCT_NAME\")\n",
    "            if env_product and env_product.strip():\n",
    "                selected_value = env_product.strip()\n",
    "            elif 'PRODUCT_NAME' in globals() and PRODUCT_NAME not in (None, \"\", \"None\"):\n",
    "                selected_value = str(PRODUCT_NAME).strip()\n",
    "            elif 'PRODUCT_ID' in globals() and PRODUCT_ID not in (None, \"\", \"None\"):\n",
    "                selected_value = str(PRODUCT_ID).strip()\n",
    "\n",
    "            if selected_value:\n",
    "                product_cols = [\n",
    "                    'product_id', 'product_name', 'sku', 'item_id', 'item', 'product', 'item_name', 'name'\n",
    "                ]\n",
    "                existing = [c for c in product_cols if c in df_cleaned.columns]\n",
    "                if existing:\n",
    "                    # Filter where any matching column equals selected value (string compare)\n",
    "                    mask = False\n",
    "                    for c in existing:\n",
    "                        mask = mask | (df_cleaned[c].astype(str) == selected_value)\n",
    "                    before = len(df_cleaned)\n",
    "                    df_cleaned = df_cleaned[mask].copy()\n",
    "                    after = len(df_cleaned)\n",
    "                    print(f\"üîé Product filter applied on columns {existing}. Rows: {before} ‚Üí {after}\")\n",
    "\n",
    "                    # Persist filtered dataset to a temporary processed file and point CLEANED_DATA_PATH to it\n",
    "                    filtered_path = os.path.join(PROCESSED_DIR, \"cleaned_data_filtered.csv\")\n",
    "                    df_cleaned.to_csv(filtered_path, index=False)\n",
    "                    CLEANED_DATA_PATH = filtered_path\n",
    "                    print(f\"üíæ Filtered cleaned data saved to: {filtered_path}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No product identifier columns found; skipping product filter.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Product filtering skipped due to error: {e}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"‚ùå Missing uploaded file: {DATA_CLEANED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab54005e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:41.592852Z",
     "iopub.status.busy": "2025-11-01T17:46:41.592509Z",
     "iopub.status.idle": "2025-11-01T17:46:41.865921Z",
     "shell.execute_reply": "2025-11-01T17:46:41.864793Z"
    },
    "papermill": {
     "duration": 0.286174,
     "end_time": "2025-11-01T17:46:41.867482",
     "exception": false,
     "start_time": "2025-11-01T17:46:41.581308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß STEP 1: FEATURE ENGINEERING\n",
      "----------------------------------------\n",
      "üìä Loading data for feature engineering...\n",
      "‚úÖ Data loaded: 525 rows, 16 columns\n",
      "\n",
      "üöÄ Starting Complete Feature Engineering Pipeline\n",
      "============================================================\n",
      "üîÑ Creating time-based features...\n",
      "‚úÖ Created 13 time-based features\n",
      "üîÑ Creating lag features for sales_qty...\n",
      "‚úÖ Created 20 lag features\n",
      "üîÑ Creating rolling features for sales_qty...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 60 rolling features\n",
      "üîÑ Creating price-related features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 6 price-related features\n",
      "üîÑ Creating inventory features...\n",
      "‚úÖ Created 6 inventory features\n",
      "üîÑ Encoding categorical features...\n",
      "‚úÖ Created 6 categorical features\n",
      "üîÑ Creating interaction features...\n",
      "‚úÖ Created 4 interaction features\n",
      "\n",
      "‚úÖ Feature Engineering Complete!\n",
      "üìä Total Features Created: 115\n",
      "üìà Dataset Shape: (525, 67)\n",
      "üíæ Saving to: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "üíæ Saving engineered dataset to F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv...\n",
      "‚úÖ Successfully saved 525 rows to F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "‚úÖ Engineered dataset and metadata saved successfully!\n",
      "üìÅ CSV: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "üìÅ Metadata: F:\\RetailSense_Lite\\data\\processed\\feature_metadata.json\n",
      "‚úÖ Feature engineering completed!\n",
      "üìä Features created: 115\n",
      "üìÅ Data saved to: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "‚úÖ File verification: 282,884 bytes written\n",
      "üîç File exists check: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Feature Engineering Pipeline\n",
    "import os\n",
    "\n",
    "def run_feature_engineering(data_path):\n",
    "    \"\"\"Run complete feature engineering pipeline for Phase 2.\"\"\"\n",
    "    print(\"\\nüîß STEP 1: FEATURE ENGINEERING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    fe = components['feature_engineering']\n",
    "    \n",
    "    # Load and process cleaned data from Phase 1\n",
    "    fe.load_data(data_path)\n",
    "    \n",
    "    # Run complete feature engineering (time-based, categorical encoding, metrics)\n",
    "    engineered_df, feature_list = fe.run_complete_feature_engineering()\n",
    "    \n",
    "    # Define engineered output path for Phase 3 (parameterized) - ensure absolute\n",
    "    engineered_path = os.path.abspath(FEATURES_DATA_PATH)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    output_dir = os.path.dirname(engineered_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save engineered data\n",
    "    print(f\"üíæ Saving to: {engineered_path}\")\n",
    "    saved_path = fe.save_engineered_data(engineered_path)\n",
    "    \n",
    "    # CRITICAL: Verify file was actually saved\n",
    "    if not os.path.exists(saved_path):\n",
    "        raise FileNotFoundError(f\"‚ùå CRITICAL ERROR: File was not saved! Expected at: {saved_path}\")\n",
    "    \n",
    "    # Verify file has content\n",
    "    file_size = os.path.getsize(saved_path)\n",
    "    if file_size == 0:\n",
    "        raise ValueError(f\"‚ùå CRITICAL ERROR: Saved file is empty! Path: {saved_path}\")\n",
    "    \n",
    "    print(f\"‚úÖ Feature engineering completed!\")\n",
    "    print(f\"üìä Features created: {len(feature_list)}\")\n",
    "    print(f\"üìÅ Data saved to: {saved_path}\")\n",
    "    print(f\"‚úÖ File verification: {file_size:,} bytes written\")\n",
    "    print(f\"üîç File exists check: {os.path.exists(saved_path)}\")\n",
    "    \n",
    "    return engineered_df, feature_list, saved_path\n",
    "\n",
    "# Execute feature engineering on uploaded cleaned data (guarded)\n",
    "if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "    print(\"‚è≠Ô∏è Skipping feature engineering (no uploaded data).\")\n",
    "else:\n",
    "    DATA_CLEANED_PATH = CLEANED_DATA_PATH\n",
    "    engineered_data, all_features, engineered_file = run_feature_engineering(DATA_CLEANED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3593d065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:41.890429Z",
     "iopub.status.busy": "2025-11-01T17:46:41.889960Z",
     "iopub.status.idle": "2025-11-01T17:46:43.234033Z",
     "shell.execute_reply": "2025-11-01T17:46:43.233268Z"
    },
    "papermill": {
     "duration": 1.35728,
     "end_time": "2025-11-01T17:46:43.235411",
     "exception": false,
     "start_time": "2025-11-01T17:46:41.878131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà STEP 2: BASELINE MODELS (ARIMA & PROPHET)\n",
      "--------------------------------------------------\n",
      "üìÇ Loading dataset from: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "‚úÖ Data loaded: 525 records, 67 columns\n",
      "üîÑ Training ARIMA Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ARIMA trained for Apples\n",
      "   AIC: 1225.18\n",
      "   Next 4 weeks forecast: [218.13 217.87 217.87 217.87]\n",
      "üîÑ Training Prophet Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:16:42 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:16:42 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prophet trained for Apples\n",
      "   Next 4 weeks forecast: [330.15 314.24 302.4  289.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Baseline models completed! Forecasting results saved to: F:\\RetailSense_Lite\\outputs\\forecasting_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Baseline Models Training and Forecast CSV Output (Robust)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def run_baseline_models(data_path):\n",
    "    \"\"\"Train baseline ARIMA and Prophet models and save forecasting results.\"\"\"\n",
    "    print(\"\\nüìà STEP 2: BASELINE MODELS (ARIMA & PROPHET)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    baseline = components['baseline_models']\n",
    "    \n",
    "    # Load feature-engineered data\n",
    "    baseline.load_data(data_path)\n",
    "    \n",
    "    # Train ARIMA and Prophet models\n",
    "    arima_forecast = baseline.train_arima()\n",
    "    prophet_forecast = baseline.train_prophet()\n",
    "    \n",
    "    # Helper function: convert any object to DataFrame with Predicted column\n",
    "    def to_dataframe(pred, model_name):\n",
    "        if isinstance(pred, pd.DataFrame):\n",
    "            df = pred.copy()\n",
    "        elif isinstance(pred, pd.Series):\n",
    "            df = pd.DataFrame({'Predicted': pred.values}, index=pred.index)\n",
    "        else:  # assume NumPy array\n",
    "            df = pd.DataFrame({'Predicted': pred})\n",
    "        df['Model'] = model_name\n",
    "        return df\n",
    "    \n",
    "    # Convert forecasts to DataFrames\n",
    "    arima_df = to_dataframe(arima_forecast, 'ARIMA')\n",
    "    prophet_df = to_dataframe(prophet_forecast, 'Prophet')\n",
    "    \n",
    "    # Combine results\n",
    "    results_df = pd.concat([arima_df, prophet_df], ignore_index=True)\n",
    "    \n",
    "    # Reset index to have a Date column if needed\n",
    "    if 'index' in results_df.columns:\n",
    "        results_df = results_df.reset_index().rename(columns={'index': 'Date'})\n",
    "    \n",
    "    # Add RMSE if available\n",
    "    if hasattr(baseline, 'rmse_scores'):\n",
    "        results_df['RMSE'] = results_df['Model'].map(baseline.rmse_scores)\n",
    "    \n",
    "    # Ensure outputs directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Save CSV for app\n",
    "    forecast_csv_path = FORECAST_CSV\n",
    "    results_df.to_csv(forecast_csv_path, index=False)\n",
    "    \n",
    "    # Optional: visualize forecasts\n",
    "    baseline.plot_forecasts()\n",
    "    \n",
    "    print(f\"‚úÖ Baseline models completed! Forecasting results saved to: {forecast_csv_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run baseline models pipeline (guarded)\n",
    "if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "    print(\"‚è≠Ô∏è Skipping baseline models (no uploaded data).\")\n",
    "else:\n",
    "    baseline_results = run_baseline_models(engineered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65995cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:43.261137Z",
     "iopub.status.busy": "2025-11-01T17:46:43.260845Z",
     "iopub.status.idle": "2025-11-01T17:46:46.407013Z",
     "shell.execute_reply": "2025-11-01T17:46:46.406195Z"
    },
    "papermill": {
     "duration": 3.161422,
     "end_time": "2025-11-01T17:46:46.408635",
     "exception": false,
     "start_time": "2025-11-01T17:46:43.247213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STEP 3: ADVANCED FORECASTING (XGBOOST & LIGHTGBM)\n",
      "------------------------------------------------------------\n",
      "üìä Loading engineered dataset for advanced forecasting from: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "‚úÖ Data loaded: 525 records, 67 columns\n",
      "üöÄ Training advanced forecasting models...\n",
      "‚úÖ Features prepared: 58 numeric features\n",
      "üìä Training set: 420, Test set: 105\n",
      "üîÑ Training XGBoost with early stopping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost - RMSE: 5.47, MAE: 3.78, MAPE: 4.0% (stopped at 221 iterations)\n",
      "üîÑ Training LightGBM with early stopping...\n",
      "‚ÑπÔ∏è LightGBM version: 4.6.0\n",
      "‚úÖ LightGBM - RMSE: 9.40, MAE: 6.35, MAPE: 7.1% (stopped at 188 iterations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced forecasting completed successfully! Results saved to: F:\\RetailSense_Lite\\outputs\\forecasting_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Advanced Forecasting Models (XGBoost & LightGBM) with CSV export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error (better than MAPE for 0 values).\"\"\"\n",
    "    return 100 * np.mean(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)\n",
    "    )\n",
    "\n",
    "def resolve_current_product():\n",
    "    env_p = os.getenv(\"PRODUCT_NAME\")\n",
    "    if env_p and env_p.strip():\n",
    "        return env_p.strip()\n",
    "    if 'PRODUCT_NAME' in globals() and PRODUCT_NAME not in (None, \"\", \"None\"):\n",
    "        return str(PRODUCT_NAME)\n",
    "    try:\n",
    "        if 'engineered_data' in globals() and 'product_name' in engineered_data.columns:\n",
    "            vals = engineered_data['product_name'].dropna().astype(str).unique().tolist()\n",
    "            if len(vals) == 1:\n",
    "                return vals[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def run_advanced_forecasting(data_path):\n",
    "    \"\"\"Train XGBoost and LightGBM models with sMAPE evaluation and save CSV.\"\"\"\n",
    "    print(\"\\nüöÄ STEP 3: ADVANCED FORECASTING (XGBOOST & LIGHTGBM)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    forecaster = components['advanced_forecasting']\n",
    "\n",
    "    # --- Load and prepare feature-engineered data ---\n",
    "    forecaster.load_and_prepare_data(data_path)\n",
    "\n",
    "    # --- Train models ---\n",
    "    results = forecaster.train_models()\n",
    "\n",
    "    # --- Compute sMAPE ---\n",
    "    for model_name in [\"xgb\", \"lgb\"]:\n",
    "        if f\"{model_name}_pred\" in results and \"y_test\" in results:\n",
    "            results[f\"{model_name}_metrics\"][\"sMAPE\"] = smape(results[\"y_test\"], results[f\"{model_name}_pred\"])\n",
    "            results[f\"{model_name}_metrics\"].pop(\"MAPE\", None)\n",
    "\n",
    "    # --- Visualizations ---\n",
    "    forecaster.plot_feature_importance()\n",
    "    forecaster.plot_predictions()\n",
    "\n",
    "    # --- Ensure outputs directory exists ---\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Save trained models ---\n",
    "    if hasattr(forecaster, 'xgb_model'):\n",
    "        joblib.dump(forecaster.xgb_model, os.path.join(OUTPUT_DIR, \"xgboost_model.pkl\"))\n",
    "    if hasattr(forecaster, 'lgb_model'):\n",
    "        joblib.dump(forecaster.lgb_model, os.path.join(OUTPUT_DIR, \"lightgbm_model.pkl\"))\n",
    "\n",
    "    # --- Save CSV with metrics for dashboard ---\n",
    "    current_product = resolve_current_product()\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Product\": [current_product, current_product],\n",
    "        \"Model\": [\"XGBoost\", \"LightGBM\"],\n",
    "        \"RMSE\": [results.get(\"xgb_metrics\", {}).get(\"RMSE\", np.nan), results.get(\"lgb_metrics\", {}).get(\"RMSE\", np.nan)],\n",
    "        \"MAE\": [results.get(\"xgb_metrics\", {}).get(\"MAE\", np.nan), results.get(\"lgb_metrics\", {}).get(\"MAE\", np.nan)],\n",
    "        \"sMAPE\": [results.get(\"xgb_metrics\", {}).get(\"sMAPE\", np.nan), results.get(\"lgb_metrics\", {}).get(\"sMAPE\", np.nan)]\n",
    "    })\n",
    "\n",
    "    metrics_csv_path = FORECAST_CSV\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Advanced forecasting completed successfully! Results saved to: {metrics_csv_path}\")\n",
    "    if current_product:\n",
    "        print(f\"üìå Forecast results tagged for product: {current_product}\")\n",
    "    return results\n",
    "\n",
    "# Run advanced forecasting pipeline (guarded)\n",
    "if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "    print(\"‚è≠Ô∏è Skipping advanced forecasting (no uploaded data).\")\n",
    "else:\n",
    "    forecasting_results = run_advanced_forecasting(engineered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d39ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:46.453529Z",
     "iopub.status.busy": "2025-11-01T17:46:46.453067Z",
     "iopub.status.idle": "2025-11-01T17:46:48.330979Z",
     "shell.execute_reply": "2025-11-01T17:46:48.329777Z"
    },
    "papermill": {
     "duration": 1.902896,
     "end_time": "2025-11-01T17:46:48.333028",
     "exception": false,
     "start_time": "2025-11-01T17:46:46.430132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç STEP 4: ANOMALY DETECTION\n",
      "----------------------------------------\n",
      "üìä Loading data for anomaly detection...\n",
      "‚úÖ Data loaded: 525 records, 67 columns\n",
      "üîÑ Preparing features for anomaly detection...\n",
      "‚úÖ Features prepared: 59 numeric features ‚Üí 2 PCA components\n",
      "üîÑ Training Isolation Forest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training One-Class SVM...\n",
      "‚úÖ Models trained and anomaly flags added\n",
      "üìà Generating anomaly dashboard...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Anomaly dashboard saved to F:\\RetailSense_Lite\\outputs\\anomaly_dashboard.png\n",
      "‚úÖ Models saved to F:\\RetailSense_Lite\\outputs\n",
      "\n",
      "‚úÖ Anomaly Detection Completed Successfully!\n",
      "   ‚Ä¢ Isolation Forest anomalies: 27\n",
      "üìÅ Anomalies saved to: F:\\RetailSense_Lite\\outputs\\anomalies.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Anomaly Detection Models (With Default Visualization)\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def run_anomaly_detection(data_path, visualize=True):\n",
    "    \"\"\"Run anomaly detection pipeline and save anomalies.csv for dashboard.\"\"\"\n",
    "    print(\"\\nüîç STEP 4: ANOMALY DETECTION\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    detector = components['anomaly_detection']  # Components already initialized\n",
    "\n",
    "    # Load feature-engineered data\n",
    "    detector.load_data(data_path)\n",
    "\n",
    "    # Prepare features for anomaly detection\n",
    "    X, X_scaled, X_pca, feature_names = detector.prepare_features()\n",
    "\n",
    "    # Run anomaly detection models (Isolation Forest, One-Class SVM, etc.)\n",
    "    detector.run_models(X_scaled, X_pca)\n",
    "\n",
    "    # Optional visualization (default is True)\n",
    "    if visualize:\n",
    "        if hasattr(detector, \"visualize_dashboard\"):\n",
    "            detector.visualize_dashboard()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No visualize_dashboard() method found in anomaly_detection component.\")\n",
    "\n",
    "    # Ensure outputs directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Build structured anomalies DataFrame ---\n",
    "    anomalies_df = pd.DataFrame({\n",
    "        \"week_start\": detector.df['week_start'] if 'week_start' in detector.df else detector.df.index,\n",
    "        \"sales_qty\": detector.df['sales_qty'] if 'sales_qty' in detector.df else detector.df.iloc[:,0],\n",
    "        \"is_anomaly_iforest\": detector.df['is_anomaly_iforest'] if 'is_anomaly_iforest' in detector.df else pd.Series([False]*len(detector.df))\n",
    "    })\n",
    "\n",
    "    # Save CSV for dashboard\n",
    "    anomaly_csv_path = ANOMALIES_CSV\n",
    "    anomalies_df.to_csv(anomaly_csv_path, index=False)\n",
    "\n",
    "    # --- Save anomaly detection models safely ---\n",
    "    if hasattr(detector, \"save_models\"):\n",
    "        detector.save_models()\n",
    "    else:\n",
    "        if hasattr(detector, \"iforest\"):\n",
    "            joblib.dump(detector.iforest, os.path.join(OUTPUT_DIR, \"isolation_forest_model.pkl\"))\n",
    "        if hasattr(detector, \"ocsvm\"):\n",
    "            joblib.dump(detector.ocsvm, os.path.join(OUTPUT_DIR, \"ocsvm_model.pkl\"))\n",
    "        print(\"‚úÖ Models saved manually\")\n",
    "\n",
    "    # Build summary of results\n",
    "    total_anomalies_iforest = anomalies_df['is_anomaly_iforest'].sum()\n",
    "    total_records = anomalies_df.shape[0]\n",
    "\n",
    "    results = {\n",
    "        \"features_used\": feature_names,\n",
    "        \"total_records\": int(total_records),\n",
    "        \"total_anomalies_iforest\": int(total_anomalies_iforest)\n",
    "    }\n",
    "\n",
    "    print(\"\\n‚úÖ Anomaly Detection Completed Successfully!\")\n",
    "    print(f\"   ‚Ä¢ Isolation Forest anomalies: {results['total_anomalies_iforest']}\")\n",
    "    print(f\"üìÅ Anomalies saved to: {anomaly_csv_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run anomaly detection pipeline (guarded)\n",
    "if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "    print(\"‚è≠Ô∏è Skipping anomaly detection (no uploaded data).\")\n",
    "else:\n",
    "    anomaly_results = run_anomaly_detection(FEATURES_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76fdf3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:48.365811Z",
     "iopub.status.busy": "2025-11-01T17:46:48.365358Z",
     "iopub.status.idle": "2025-11-01T17:46:48.978806Z",
     "shell.execute_reply": "2025-11-01T17:46:48.977993Z"
    },
    "papermill": {
     "duration": 0.628542,
     "end_time": "2025-11-01T17:46:48.980271",
     "exception": false,
     "start_time": "2025-11-01T17:46:48.351729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 5: MODEL PERFORMANCE COMPARISON\n",
      "---------------------------------------------\n",
      "\n",
      "üéØ FORECASTING MODEL PERFORMANCE:\n",
      "          Model Type Model Name     RMSE      MAE    sMAPE\n",
      "Advanced Forecasting    XGBoost 5.472354 3.784855 3.340859\n",
      "Advanced Forecasting   LightGBM 9.396908 6.345216 5.656022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Performance comparison chart saved at: F:\\RetailSense_Lite\\outputs\\model_performance_comparison.png\n",
      "\n",
      "üö® ANOMALY DETECTION SUMMARY:\n",
      "  ‚Ä¢ Isolation Forest: 27 anomalies\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 7: Model Performance Comparison (Fixed)\n",
    "# -------------------\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_model_performance(forecasting_results, anomaly_results, visualize=True):\n",
    "    print(\"\\nüìä STEP 5: MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    performance_summary = {'Model Type': [], 'Model Name': [], 'RMSE': [], 'MAE': [], 'sMAPE': []}\n",
    "\n",
    "    # Handle dict output from run_advanced_forecasting\n",
    "    if isinstance(forecasting_results, dict):\n",
    "        for model_key, display_name in [('xgb_metrics', 'XGBoost'), ('lgb_metrics', 'LightGBM')]:\n",
    "            if model_key in forecasting_results:\n",
    "                metrics = forecasting_results[model_key]\n",
    "                performance_summary['Model Type'].append('Advanced Forecasting')\n",
    "                performance_summary['Model Name'].append(display_name)\n",
    "                performance_summary['RMSE'].append(metrics.get('RMSE', np.nan))\n",
    "                performance_summary['MAE'].append(metrics.get('MAE', np.nan))\n",
    "                performance_summary['sMAPE'].append(metrics.get('sMAPE', np.nan))\n",
    "\n",
    "    # Handle DataFrame (optional, e.g., loaded from CSV)\n",
    "    elif isinstance(forecasting_results, pd.DataFrame) and not forecasting_results.empty:\n",
    "        for idx, row in forecasting_results.iterrows():\n",
    "            performance_summary['Model Type'].append('Advanced Forecasting')\n",
    "            performance_summary['Model Name'].append(row['Model'])\n",
    "            performance_summary['RMSE'].append(row.get('RMSE', np.nan))\n",
    "            performance_summary['MAE'].append(row.get('MAE', np.nan))\n",
    "            performance_summary['sMAPE'].append(row.get('sMAPE', np.nan))\n",
    "\n",
    "    perf_df = pd.DataFrame(performance_summary)\n",
    "\n",
    "    if not perf_df.empty:\n",
    "        print(\"\\nüéØ FORECASTING MODEL PERFORMANCE:\")\n",
    "        print(perf_df.to_string(index=False))\n",
    "\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "            metrics = ['RMSE', 'MAE', 'sMAPE']\n",
    "            for i, metric in enumerate(metrics):\n",
    "                axes[i].bar(perf_df['Model Name'], perf_df[metric], alpha=0.7)\n",
    "                axes[i].set_title(f'{metric} Comparison')\n",
    "                axes[i].set_ylabel(metric)\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save figure\n",
    "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "            fig_path = os.path.join(OUTPUT_DIR, 'model_performance_comparison.png')\n",
    "            plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"‚úÖ Performance comparison chart saved at: {fig_path}\")\n",
    "\n",
    "    # Anomaly summary\n",
    "    if anomaly_results is not None:\n",
    "        print(\"\\nüö® ANOMALY DETECTION SUMMARY:\")\n",
    "        if 'total_anomalies_iforest' in anomaly_results:\n",
    "            print(f\"  ‚Ä¢ Isolation Forest: {anomaly_results['total_anomalies_iforest']} anomalies\")\n",
    "        if 'total_anomalies_ocsvm' in anomaly_results:\n",
    "            print(f\"  ‚Ä¢ One-Class SVM: {anomaly_results.get('total_anomalies_ocsvm', 0)} anomalies\")\n",
    "\n",
    "# Execute comparison with default plotting (guarded)\n",
    "if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "    print(\"‚è≠Ô∏è Skipping model performance comparison (no uploaded data).\")\n",
    "else:\n",
    "    compare_model_performance(forecasting_results, anomaly_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee80b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:49.010469Z",
     "iopub.status.busy": "2025-11-01T17:46:49.010077Z",
     "iopub.status.idle": "2025-11-01T17:46:49.042487Z",
     "shell.execute_reply": "2025-11-01T17:46:49.041733Z"
    },
    "papermill": {
     "duration": 0.049686,
     "end_time": "2025-11-01T17:46:49.044171",
     "exception": false,
     "start_time": "2025-11-01T17:46:48.994485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° STEP 6: BUSINESS INSIGHTS GENERATION\n",
      "-------------------------------------------------------\n",
      "‚ö†Ô∏è Could not load feature importance: All arrays must be of the same length\n",
      "\n",
      "üíº BUSINESS INSIGHTS:\n",
      "  1. üéØ FORECASTING: XGBoost performed best with sMAPE=3.34%\n",
      "  2. ‚úÖ Forecasting accuracy is EXCELLENT (sMAPE < 15%) - ready for production\n",
      "  3. üö® Isolation Forest flagged 27 unusual records\n",
      "\n",
      "‚úÖ Insights report saved at: F:\\RetailSense_Lite\\outputs\\phase2_business_insights.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 8: Business Insights Generation (Updated)\n",
    "# -------------------\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def generate_business_insights():\n",
    "    \"\"\"Generate comprehensive business insights from all models\"\"\"\n",
    "    print(\"\\nüí° STEP 6: BUSINESS INSIGHTS GENERATION\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    insights = []\n",
    "\n",
    "    # --- Forecasting insights ---\n",
    "    if 'forecasting_results' in globals() and forecasting_results and 'xgb_metrics' in forecasting_results and 'lgb_metrics' in forecasting_results:\n",
    "        xgb_smape = forecasting_results['xgb_metrics'].get('sMAPE', 999)\n",
    "        lgb_smape = forecasting_results['lgb_metrics'].get('sMAPE', 999)\n",
    "\n",
    "        best_model = \"XGBoost\" if xgb_smape < lgb_smape else \"LightGBM\"\n",
    "        best_smape = min(xgb_smape, lgb_smape)\n",
    "\n",
    "        insights.append(f\"üéØ FORECASTING: {best_model} performed best with sMAPE={best_smape:.2f}%\")\n",
    "\n",
    "        if best_smape < 15:\n",
    "            insights.append(\"‚úÖ Forecasting accuracy is EXCELLENT (sMAPE < 15%) - ready for production\")\n",
    "        elif best_smape < 25:\n",
    "            insights.append(\"‚ö†Ô∏è Forecasting accuracy is GOOD (sMAPE < 25%) - some fine-tuning recommended\")\n",
    "        else:\n",
    "            insights.append(\"üîß Forecasting accuracy is WEAK (sMAPE > 25%) - revisit feature engineering\")\n",
    "\n",
    "    # --- Anomaly insights ---\n",
    "    if 'anomaly_results' in globals() and anomaly_results:\n",
    "        if 'total_anomalies_iforest' in anomaly_results:\n",
    "            insights.append(f\"üö® Isolation Forest flagged {anomaly_results['total_anomalies_iforest']} unusual records\")\n",
    "        if 'total_anomalies_ocsvm' in anomaly_results:\n",
    "            insights.append(f\"üö® One-Class SVM flagged {anomaly_results['total_anomalies_ocsvm']} unusual records\")\n",
    "\n",
    "    # --- Feature importance insights ---\n",
    "    try:\n",
    "        xgb_model_path = os.path.join(OUTPUT_DIR, \"xgboost_model.pkl\")\n",
    "        if os.path.exists(xgb_model_path) and 'all_features' in globals():\n",
    "            xgb_model = joblib.load(xgb_model_path)\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': all_features,\n",
    "                'importance': xgb_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "            top_features = feature_importance.head(3)['feature'].tolist()\n",
    "            insights.append(f\"üîë TOP PREDICTIVE FEATURES: {', '.join(top_features)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load feature importance: {e}\")\n",
    "\n",
    "    # --- Save insights ---\n",
    "    insights_report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_insights': len(insights),\n",
    "        'insights': insights\n",
    "    }\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    output_file = os.path.join(OUTPUT_DIR, \"phase2_business_insights.csv\")\n",
    "    pd.DataFrame([insights_report]).to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"\\nüíº BUSINESS INSIGHTS:\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"  {i}. {insight}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Insights report saved at: {output_file}\")\n",
    "    return insights\n",
    "\n",
    "# Run business insights generation (guarded)\n",
    "if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "    print(\"‚è≠Ô∏è Skipping business insights (no uploaded data).\")\n",
    "else:\n",
    "    business_insights = generate_business_insights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad9e16b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:49.094934Z",
     "iopub.status.busy": "2025-11-01T17:46:49.094482Z",
     "iopub.status.idle": "2025-11-01T17:46:49.109243Z",
     "shell.execute_reply": "2025-11-01T17:46:49.108461Z"
    },
    "papermill": {
     "duration": 0.041892,
     "end_time": "2025-11-01T17:46:49.110888",
     "exception": false,
     "start_time": "2025-11-01T17:46:49.068996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä DATASET PROCESSED:\n",
      "   ‚Ä¢ Engineered shape: (525, 67)\n",
      "   ‚Ä¢ Features created: 115\n",
      "   ‚Ä¢ Time range: 2023-10-30 00:00:00 ‚Üí 2025-10-27 00:00:00\n",
      "\n",
      "ü§ñ MODELS TRAINED:\n",
      "   ‚úÖ Baseline Models: ARIMA, Prophet\n",
      "   ‚úÖ Advanced Models: XGBoost, LightGBM\n",
      "   ‚úÖ Anomaly Detection: Isolation Forest, One-Class SVM\n",
      "\n",
      "üéØ BEST FORECASTING PERFORMANCE:\n",
      "   ‚Ä¢ XGBoost with 3.34% sMAPE\n",
      "\n",
      "üö® ANOMALY DETECTION SUMMARY:\n",
      "   ‚Ä¢ Isolation Forest: 27 anomalies\n",
      "   ‚Ä¢ One-Class SVM: N/A anomalies\n",
      "\n",
      "üìÅ FILES GENERATED:\n",
      "   ‚úÖ F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "   ‚úÖ F:\\RetailSense_Lite\\outputs\\model_performance_comparison.png\n",
      "   ‚úÖ F:\\RetailSense_Lite\\outputs\\phase2_business_insights.csv\n",
      "   ‚úÖ F:\\RetailSense_Lite\\outputs\\forecasting_results.csv\n",
      "   ‚úÖ F:\\RetailSense_Lite\\outputs\\anomalies.csv\n",
      "\n",
      "‚úÖ PHASE 2 COMPLETED SUCCESSFULLY!\n",
      "üìÑ Summary generated at: F:\\RetailSense_Lite\\outputs\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 9: Phase 2 Completion Summary (Enhanced)\n",
    "# -------------------\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def phase2_completion_summary_enhanced():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "    # If no uploaded data, do not attempt to summarize anything\n",
    "    if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "        print(\"‚ùå No uploaded data detected. Phase 2 summary is not generated to avoid stale results.\")\n",
    "        print(\"   Set 'UPLOADED_DATA_PATH' or place file at `data/uploaded/uploaded_data.csv` and rerun.\")\n",
    "        print(\"=\"*60)\n",
    "        return\n",
    "    \n",
    "    # --- Dataset Info ---\n",
    "    if 'engineered_data' in globals():\n",
    "        shape = engineered_data.shape\n",
    "        num_features = len(all_features) if 'all_features' in globals() else 'N/A'\n",
    "        time_range = f\"{engineered_data['week_start'].min()} ‚Üí {engineered_data['week_start'].max()}\" if 'week_start' in engineered_data else \"N/A\"\n",
    "        print(f\"üìä DATASET PROCESSED:\\n   ‚Ä¢ Engineered shape: {shape}\\n   ‚Ä¢ Features created: {num_features}\\n   ‚Ä¢ Time range: {time_range}\\n\")\n",
    "    \n",
    "    # --- Models Trained ---\n",
    "    print(\"ü§ñ MODELS TRAINED:\")\n",
    "    print(\"   ‚úÖ Baseline Models: ARIMA, Prophet\")\n",
    "    print(\"   ‚úÖ Advanced Models: XGBoost, LightGBM\")\n",
    "    print(\"   ‚úÖ Anomaly Detection: Isolation Forest, One-Class SVM\\n\")\n",
    "    \n",
    "    # --- Best Forecasting Performance ---\n",
    "    best_model, best_smape = \"N/A\", None\n",
    "\n",
    "    # Prefer in-memory results if available\n",
    "    if 'forecasting_results' in globals() and isinstance(forecasting_results, dict):\n",
    "        xgb_smape = forecasting_results.get('xgb_metrics', {}).get('sMAPE', np.inf)\n",
    "        lgb_smape = forecasting_results.get('lgb_metrics', {}).get('sMAPE', np.inf)\n",
    "        if np.isfinite(xgb_smape) or np.isfinite(lgb_smape):\n",
    "            if xgb_smape < lgb_smape:\n",
    "                best_model, best_smape = \"XGBoost\", xgb_smape\n",
    "            else:\n",
    "                best_model, best_smape = \"LightGBM\", lgb_smape\n",
    "\n",
    "    # Fallback: try reading metrics CSV if memory object not present\n",
    "    if best_smape is None:\n",
    "        try:\n",
    "            if 'FORECAST_CSV' in globals() and os.path.exists(FORECAST_CSV):\n",
    "                df_metrics = pd.read_csv(FORECAST_CSV)\n",
    "                if {'Model','sMAPE'}.issubset(df_metrics.columns) and not df_metrics.empty:\n",
    "                    row = df_metrics.loc[df_metrics['sMAPE'].astype(float).idxmin()]\n",
    "                    best_model = str(row['Model'])\n",
    "                    best_smape = float(row['sMAPE'])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if best_smape is not None and np.isfinite(best_smape):\n",
    "        print(f\"üéØ BEST FORECASTING PERFORMANCE:\\n   ‚Ä¢ {best_model} with {best_smape:.2f}% sMAPE\\n\")\n",
    "    else:\n",
    "        print(\"üéØ BEST FORECASTING PERFORMANCE:\\n   ‚Ä¢ N/A\\n\")\n",
    "    \n",
    "    # --- Anomaly Detection Summary ---\n",
    "    iso_anom = anomaly_results.get('total_anomalies_iforest', 'N/A') if 'anomaly_results' in globals() else 'N/A'\n",
    "    svm_anom = anomaly_results.get('total_anomalies_ocsvm', 'N/A') if 'anomaly_results' in globals() else 'N/A'\n",
    "    print(\"üö® ANOMALY DETECTION SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Isolation Forest: {iso_anom} anomalies\")\n",
    "    print(f\"   ‚Ä¢ One-Class SVM: {svm_anom} anomalies\\n\")\n",
    "    \n",
    "    # --- Generated Files ---\n",
    "    print(\"üìÅ FILES GENERATED:\")\n",
    "    outputs_dir = OUTPUT_DIR\n",
    "    processed_file = FEATURES_DATA_PATH\n",
    "    files_list = [\n",
    "        processed_file,\n",
    "        os.path.join(outputs_dir, \"model_performance_comparison.png\"),\n",
    "        os.path.join(outputs_dir, \"phase2_business_insights.csv\"),\n",
    "        os.path.join(outputs_dir, \"forecasting_results.csv\"),\n",
    "        os.path.join(outputs_dir, \"anomalies.csv\")\n",
    "    ]\n",
    "    for f in files_list:\n",
    "        status = \"‚úÖ\" if os.path.exists(f) else \"‚ö†Ô∏è Missing\"\n",
    "        print(f\"   {status} {f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ PHASE 2 COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"üìÑ Summary generated at: {outputs_dir}\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run enhanced Phase 2 summary\n",
    "phase2_completion_summary_enhanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a59e9d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:46:49.152285Z",
     "iopub.status.busy": "2025-11-01T17:46:49.151845Z",
     "iopub.status.idle": "2025-11-01T17:46:49.160876Z",
     "shell.execute_reply": "2025-11-01T17:46:49.160118Z"
    },
    "papermill": {
     "duration": 0.026959,
     "end_time": "2025-11-01T17:46:49.162475",
     "exception": false,
     "start_time": "2025-11-01T17:46:49.135516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç PHASE 2 VALIDATION\n",
      "------------------------------------------------------------\n",
      "‚úÖ Feature engineering file found: F:\\RetailSense_Lite\\data\\processed\\data_with_all_features.csv\n",
      "‚úÖ Baseline models ran successfully (ARIMA, Prophet)\n",
      "‚úÖ Advanced forecasting models ran successfully (XGBoost, LightGBM)\n",
      "‚úÖ Anomaly detection models ran successfully (Isolation Forest, One-Class SVM)\n",
      "‚úÖ All key output files generated\n",
      "\n",
      "------------------------------------------------------------\n",
      "üéâ ALL VALIDATIONS PASSED! ‚úÖ\n",
      "üöÄ Ready to proceed to Phase 3: Business Layer & Dashboard\n",
      "------------------------------------------------------------\n",
      "üìÖ Validation completed at: 2025-11-01 23:16:49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Cell 10: Phase 2 Validation and Testing (Corrected)\n",
    "# -------------------\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def validate_phase2_outputs_enhanced():\n",
    "    \"\"\"Validate all Phase 2 outputs before moving to Phase 3\"\"\"\n",
    "    print(\"\\nüîç PHASE 2 VALIDATION\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Short-circuit when no uploaded data is present\n",
    "    if 'SKIP_PHASE2' in globals() and SKIP_PHASE2:\n",
    "        print(\"‚ùå Validation skipped: no uploaded data file detected.\")\n",
    "        print(\"   Set 'UPLOADED_DATA_PATH' or place file at `data/uploaded/uploaded_data.csv` and rerun Phase 2.\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"üìÖ Validation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        return False\n",
    "\n",
    "    outputs_dir = OUTPUT_DIR\n",
    "    processed_dir = PROCESSED_DIR\n",
    "\n",
    "    validation_results = {\n",
    "        \"feature_engineering\": False,\n",
    "        \"baseline_models\": False,\n",
    "        \"advanced_forecasting\": False,\n",
    "        \"anomaly_detection\": False,\n",
    "        \"outputs\": False\n",
    "    }\n",
    "\n",
    "    # --- Feature engineering validation ---\n",
    "    fe_file = FEATURES_DATA_PATH\n",
    "    if os.path.exists(fe_file):\n",
    "        validation_results[\"feature_engineering\"] = True\n",
    "        print(f\"‚úÖ Feature engineering file found: {fe_file}\")\n",
    "\n",
    "    # --- Baseline models validation ---\n",
    "    if 'baseline_results' in globals() and isinstance(baseline_results, pd.DataFrame) and not baseline_results.empty:\n",
    "        validation_results[\"baseline_models\"] = True\n",
    "        print(\"‚úÖ Baseline models ran successfully (ARIMA, Prophet)\")\n",
    "\n",
    "    # --- Advanced forecasting validation ---\n",
    "    if 'forecasting_results' in globals() and isinstance(forecasting_results, dict) and \"xgb_metrics\" in forecasting_results:\n",
    "        validation_results[\"advanced_forecasting\"] = True\n",
    "        print(\"‚úÖ Advanced forecasting models ran successfully (XGBoost, LightGBM)\")\n",
    "\n",
    "    # --- Anomaly detection validation ---\n",
    "    if 'anomaly_results' in globals() and isinstance(anomaly_results, dict) and 'total_anomalies_iforest' in anomaly_results:\n",
    "        validation_results[\"anomaly_detection\"] = True\n",
    "        print(\"‚úÖ Anomaly detection models ran successfully (Isolation Forest, One-Class SVM)\")\n",
    "\n",
    "    # --- Outputs validation ---\n",
    "    output_files = [\n",
    "        os.path.join(outputs_dir, \"model_performance_comparison.png\"),\n",
    "        os.path.join(outputs_dir, \"phase2_business_insights.csv\"),\n",
    "        os.path.join(outputs_dir, \"forecasting_results.csv\"),\n",
    "        os.path.join(outputs_dir, \"anomalies.csv\")\n",
    "    ]\n",
    "    missing_files = [f for f in output_files if not os.path.exists(f)]\n",
    "    if not missing_files:\n",
    "        validation_results[\"outputs\"] = True\n",
    "        print(\"‚úÖ All key output files generated\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Missing output files:\")\n",
    "        for f in missing_files:\n",
    "            print(f\"   ‚ùå {f}\")\n",
    "\n",
    "    # --- Final check ---\n",
    "    all_passed = all(validation_results.values())\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    if all_passed:\n",
    "        print(\"üéâ ALL VALIDATIONS PASSED! ‚úÖ\")\n",
    "        print(\"üöÄ Ready to proceed to Phase 3: Business Layer & Dashboard\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some validations failed:\")\n",
    "        for comp, status in validation_results.items():\n",
    "            if not status:\n",
    "                print(f\"   ‚ùå {comp} FAILED\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"üìÖ Validation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "# --- Run enhanced validation ---\n",
    "validation_passed = validate_phase2_outputs_enhanced()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retailsense_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.5242,
   "end_time": "2025-11-01T17:46:50.485229",
   "environment_variables": {},
   "exception": null,
   "input_path": "F:\\RetailSense_Lite\\notebooks\\phase2_core_models.ipynb",
   "output_path": "F:\\RetailSense_Lite\\notebooks\\phase2_out.ipynb",
   "parameters": {},
   "start_time": "2025-11-01T17:46:33.961029",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}