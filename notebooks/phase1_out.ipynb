{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec83250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:17.308157Z",
     "iopub.status.busy": "2025-11-02T20:56:17.307623Z",
     "iopub.status.idle": "2025-11-02T20:56:17.313058Z",
     "shell.execute_reply": "2025-11-02T20:56:17.312150Z"
    },
    "papermill": {
     "duration": 0.01691,
     "end_time": "2025-11-02T20:56:17.315585",
     "exception": false,
     "start_time": "2025-11-02T20:56:17.298675",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "INPUT_CSV = \"F:\\\\RetailSense_Lite\\\\data\\\\uploaded\\\\uploaded_data.csv\"\n",
    "RETAILSENSE_BASE_DIR = \"F:\\\\RetailSense_Lite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29117f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:17.341660Z",
     "iopub.status.busy": "2025-11-02T20:56:17.341209Z",
     "iopub.status.idle": "2025-11-02T20:56:17.355212Z",
     "shell.execute_reply": "2025-11-02T20:56:17.354386Z"
    },
    "papermill": {
     "duration": 0.023942,
     "end_time": "2025-11-02T20:56:17.356854",
     "exception": false,
     "start_time": "2025-11-02T20:56:17.332912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Phase 1 outputs cleared (raw2.csv, cleaned_data.csv)\n",
      "‚úÖ Using uploaded data file for Phase 1: F:\\RetailSense_Lite\\data\\uploaded\\uploaded_data.csv\n",
      "üìÅ BASE_DIR: F:\\RetailSense_Lite\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS & PATHS (Papermill-friendly) + Upload Gate\n",
    "# This cell defines configurable inputs and base directories.\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Base directory\n",
    "if 'RETAILSENSE_BASE_DIR' not in globals() or not RETAILSENSE_BASE_DIR:\n",
    "    RETAILSENSE_BASE_DIR = r\"F:\\RetailSense_Lite\"\n",
    "\n",
    "BASE_DIR = RETAILSENSE_BASE_DIR\n",
    "RAW_DIR = os.path.join(BASE_DIR, 'data', 'raw')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'outputs')\n",
    "UPLOAD_DIR = os.path.join(BASE_DIR, 'data', 'uploaded')\n",
    "DEFAULT_UPLOAD_PATH = os.path.join(UPLOAD_DIR, 'uploaded_data.csv')\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Uploaded data path gate (env override allowed)\n",
    "UPLOADED_DATA_PATH = os.getenv('UPLOADED_DATA_PATH')\n",
    "if not UPLOADED_DATA_PATH or not UPLOADED_DATA_PATH.strip():\n",
    "    UPLOADED_DATA_PATH = DEFAULT_UPLOAD_PATH\n",
    "UPLOADED_DATA_PATH = os.path.normpath(UPLOADED_DATA_PATH)\n",
    "\n",
    "# For Phase 1, INPUT_CSV must be the uploaded file\n",
    "INPUT_CSV = UPLOADED_DATA_PATH\n",
    "\n",
    "# Utility: Clear Phase 1 outputs (avoid stale results)\n",
    "def clear_phase1_outputs():\n",
    "    try:\n",
    "        # Remove Phase 1 processed outputs\n",
    "        for fname in [\n",
    "            os.path.join(PROCESSED_DIR, 'raw2.csv'),\n",
    "            os.path.join(PROCESSED_DIR, 'cleaned_data.csv')\n",
    "        ]:\n",
    "            if os.path.exists(fname):\n",
    "                os.remove(fname)\n",
    "        print(\"üßπ Phase 1 outputs cleared (raw2.csv, cleaned_data.csv)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not clear some Phase 1 outputs: {e}\")\n",
    "\n",
    "# Determine gate status\n",
    "SKIP_PHASE1 = False\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    clear_phase1_outputs()\n",
    "    SKIP_PHASE1 = True\n",
    "    print(\"‚ùå No uploaded data file found for Phase 1.\")\n",
    "    print(\"   Set env 'UPLOADED_DATA_PATH' or place file at: \" + DEFAULT_UPLOAD_PATH)\n",
    "    print(\"   Phase 1 execution will be skipped to avoid stale results.\")\n",
    "else:\n",
    "    # Clear any previous outputs to ensure fresh run\n",
    "    clear_phase1_outputs()\n",
    "    print(f\"‚úÖ Using uploaded data file for Phase 1: {INPUT_CSV}\")\n",
    "\n",
    "print(f\"üìÅ BASE_DIR: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e074c1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:17.378495Z",
     "iopub.status.busy": "2025-11-02T20:56:17.378033Z",
     "iopub.status.idle": "2025-11-02T20:56:21.645167Z",
     "shell.execute_reply": "2025-11-02T20:56:21.644348Z"
    },
    "papermill": {
     "duration": 4.278571,
     "end_time": "2025-11-02T20:56:21.646536",
     "exception": false,
     "start_time": "2025-11-02T20:56:17.367965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõí RetailSense Lite - Custom Dataset Analysis\n",
      "============================================================\n",
      "‚úÖ Dataset loaded: (525, 16)\n",
      "\n",
      "üìä Column Names:\n",
      "['product_id', 'product_name', 'category', 'week_no', 'week_start', 'week_end', 'sales_qty', 'stock_on_hand', 'availability', 'price', 'promotion', 'holiday_flag', 'season', 'weather', 'disaster_flag', 'expiry_date']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Load Your Dataset (Guarded by upload gate)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üõí RetailSense Lite - Custom Dataset Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Upload gate\n",
    "if 'SKIP_PHASE1' in globals() and SKIP_PHASE1:\n",
    "    print(\"‚è≠Ô∏è Skipping Phase 1: No uploaded data file.\")\n",
    "else:\n",
    "    # Load your dataset using uploaded INPUT_CSV\n",
    "    if not os.path.exists(INPUT_CSV):\n",
    "        raise FileNotFoundError(f\"Input CSV not found: {INPUT_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    print(f\"‚úÖ Dataset loaded: {df.shape}\")\n",
    "    print(\"\\nüìä Column Names:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e509235a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:21.665723Z",
     "iopub.status.busy": "2025-11-02T20:56:21.665438Z",
     "iopub.status.idle": "2025-11-02T20:56:21.678939Z",
     "shell.execute_reply": "2025-11-02T20:56:21.678163Z"
    },
    "papermill": {
     "duration": 0.025118,
     "end_time": "2025-11-02T20:56:21.680222",
     "exception": false,
     "start_time": "2025-11-02T20:56:21.655104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõçÔ∏è Available Products: 5\n",
      "['Milk' 'Bread' 'Apples' 'Shampoo' 'Rice']\n",
      "‚úÖ Using default product: Milk\n",
      "‚ÑπÔ∏è 'date' column not found; skipping time series plot.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Product Selection for Forecasting\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if 'SKIP_PHASE1' in globals() and SKIP_PHASE1:\n",
    "    print(\"‚è≠Ô∏è Skipping product selection (no uploaded data).\")\n",
    "else:\n",
    "    file_path = INPUT_CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    if 'product_name' in df.columns:\n",
    "        unique_products = df['product_name'].unique()\n",
    "        print(f\"üõçÔ∏è Available Products: {len(unique_products)}\")\n",
    "        print(unique_products[:20])\n",
    "        env_input = os.environ.get(\"PRODUCT_NAME\", \"\").strip()\n",
    "        if env_input in unique_products:\n",
    "            selected_product = env_input\n",
    "        else:\n",
    "            selected_product = unique_products[0]\n",
    "            if env_input != \"\":\n",
    "                print(f\"‚ö†Ô∏è Product '{env_input}' not found. Using default: {selected_product}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Using default product: {selected_product}\")\n",
    "        product_df = df[df['product_name'] == selected_product].copy()\n",
    "        if 'date' in product_df.columns:\n",
    "            agg_cols = {\"sales_qty\": \"sum\"}\n",
    "            if 'revenue' in product_df.columns:\n",
    "                agg_cols[\"revenue\"] = \"sum\"\n",
    "            sales_by_date = (\n",
    "                product_df.groupby('date')\n",
    "                .agg(agg_cols)\n",
    "                .reset_index()\n",
    "                .sort_values('date')\n",
    "            )\n",
    "            print(f\"üìä Filtered dataset size: {sales_by_date.shape}\")\n",
    "            print(sales_by_date.head())\n",
    "            plt.figure(figsize=(12,5))\n",
    "            plt.plot(sales_by_date['date'], sales_by_date['sales_qty'], marker='o')\n",
    "            plt.title(f\"üìà Sales Trend for {selected_product}\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Sales Quantity\")\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è 'date' column not found; skipping time series plot.\")\n",
    "    else:\n",
    "        print(\"‚ùå 'product_name' column not found in dataset. Please check the file format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb42de2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:21.702630Z",
     "iopub.status.busy": "2025-11-02T20:56:21.702196Z",
     "iopub.status.idle": "2025-11-02T20:56:21.775411Z",
     "shell.execute_reply": "2025-11-02T20:56:21.774376Z"
    },
    "papermill": {
     "duration": 0.087151,
     "end_time": "2025-11-02T20:56:21.777018",
     "exception": false,
     "start_time": "2025-11-02T20:56:21.689867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DATA OVERVIEW\n",
      "==================================================\n",
      "\n",
      "üìã Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 525 entries, 0 to 524\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   product_id     525 non-null    object \n",
      " 1   product_name   525 non-null    object \n",
      " 2   category       525 non-null    object \n",
      " 3   week_no        525 non-null    int64  \n",
      " 4   week_start     525 non-null    object \n",
      " 5   week_end       525 non-null    object \n",
      " 6   sales_qty      525 non-null    float64\n",
      " 7   stock_on_hand  525 non-null    float64\n",
      " 8   availability   525 non-null    int64  \n",
      " 9   price          525 non-null    float64\n",
      " 10  promotion      525 non-null    int64  \n",
      " 11  holiday_flag   525 non-null    int64  \n",
      " 12  season         525 non-null    object \n",
      " 13  weather        525 non-null    object \n",
      " 14  disaster_flag  525 non-null    int64  \n",
      " 15  expiry_date    525 non-null    object \n",
      "dtypes: float64(3), int64(5), object(8)\n",
      "memory usage: 65.8+ KB\n",
      "\n",
      "üìä Basic Statistics:\n",
      "               count unique         top freq        mean        std    min  \\\n",
      "product_id       525      5        P001  105         NaN        NaN    NaN   \n",
      "product_name     525      5        Milk  105         NaN        NaN    NaN   \n",
      "category         525      5       Dairy  105         NaN        NaN    NaN   \n",
      "week_no        525.0    NaN         NaN  NaN        53.0  30.338422    1.0   \n",
      "week_start       525    105  2023-10-30    5         NaN        NaN    NaN   \n",
      "week_end         525    105  2023-11-05    5         NaN        NaN    NaN   \n",
      "sales_qty      525.0    NaN         NaN  NaN  206.577524  81.391107   16.0   \n",
      "stock_on_hand  525.0    NaN         NaN  NaN  330.861333  91.935242  120.0   \n",
      "availability   525.0    NaN         NaN  NaN         1.0        0.0    1.0   \n",
      "price          525.0    NaN         NaN  NaN  171.130133  71.822027  50.01   \n",
      "promotion      525.0    NaN         NaN  NaN    0.055238   0.228662    0.0   \n",
      "holiday_flag   525.0    NaN         NaN  NaN    0.064762    0.24634    0.0   \n",
      "season           525      4      Autumn  135         NaN        NaN    NaN   \n",
      "weather          525      4       Sunny  141         NaN        NaN    NaN   \n",
      "disaster_flag  525.0    NaN         NaN  NaN    0.026667   0.161261    0.0   \n",
      "expiry_date      525    387  2024-10-13    5         NaN        NaN    NaN   \n",
      "\n",
      "                  25%     50%     75%     max  \n",
      "product_id        NaN     NaN     NaN     NaN  \n",
      "product_name      NaN     NaN     NaN     NaN  \n",
      "category          NaN     NaN     NaN     NaN  \n",
      "week_no          27.0    53.0    79.0   105.0  \n",
      "week_start        NaN     NaN     NaN     NaN  \n",
      "week_end          NaN     NaN     NaN     NaN  \n",
      "sales_qty       148.0   201.0   260.0   605.0  \n",
      "stock_on_hand   273.0   330.0   383.0   767.0  \n",
      "availability      1.0     1.0     1.0     1.0  \n",
      "price          106.69  171.51  233.78  299.36  \n",
      "promotion         0.0     0.0     0.0     1.0  \n",
      "holiday_flag      0.0     0.0     0.0     1.0  \n",
      "season            NaN     NaN     NaN     NaN  \n",
      "weather           NaN     NaN     NaN     NaN  \n",
      "disaster_flag     0.0     0.0     0.0     1.0  \n",
      "expiry_date       NaN     NaN     NaN     NaN  \n",
      "\n",
      "‚ùì Missing Values Check:\n",
      "‚úÖ No missing values found.\n",
      "üì¶ Unique Products: 5\n",
      "üè∑Ô∏è Unique Categories: 5\n",
      "üìÖ Date Range: 2023-10-30 to 2025-11-02\n",
      "\n",
      "üìù Columns in DataFrame:\n",
      "['product_id', 'product_name', 'category', 'week_no', 'week_start', 'week_end', 'sales_qty', 'stock_on_hand', 'availability', 'price', 'promotion', 'holiday_flag', 'season', 'weather', 'disaster_flag', 'expiry_date']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Overview and Quality Check (Guarded)\n",
    "import pandas as pd\n",
    "\n",
    "if 'SKIP_PHASE1' in globals() and SKIP_PHASE1:\n",
    "    print(\"‚è≠Ô∏è Skipping data overview (no uploaded data).\")\n",
    "else:\n",
    "    print(\"üîç DATA OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\nüìã Dataset Info:\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\nüìä Basic Statistics:\")\n",
    "    print(df.describe(include='all').transpose())   # include categorical stats too\n",
    "\n",
    "    print(\"\\n‚ùì Missing Values Check:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_values,\n",
    "        'Missing_Percentage': missing_percent.round(2)\n",
    "    }).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "    if missing_df['Missing_Count'].sum() == 0:\n",
    "        print(\"‚úÖ No missing values found.\")\n",
    "    else:\n",
    "        print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "    if \"store_id\" in df.columns:\n",
    "        print(f\"\\nüè™ Unique Stores: {df['store_id'].nunique()}\")\n",
    "\n",
    "    if \"product_id\" in df.columns:\n",
    "        print(f\"üì¶ Unique Products: {df['product_id'].nunique()}\")\n",
    "\n",
    "    if \"category\" in df.columns:\n",
    "        print(f\"üè∑Ô∏è Unique Categories: {df['category'].nunique()}\")\n",
    "\n",
    "    if \"week_start\" in df.columns and \"week_end\" in df.columns:\n",
    "        df[\"week_start\"] = pd.to_datetime(df[\"week_start\"], errors=\"coerce\")\n",
    "        df[\"week_end\"] = pd.to_datetime(df[\"week_end\"], errors=\"coerce\")\n",
    "        print(f\"üìÖ Date Range: {df['week_start'].min().date()} to {df['week_end'].max().date()}\")\n",
    "\n",
    "    print(\"\\nüìù Columns in DataFrame:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "366997fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:21.800400Z",
     "iopub.status.busy": "2025-11-02T20:56:21.800113Z",
     "iopub.status.idle": "2025-11-02T20:56:21.820103Z",
     "shell.execute_reply": "2025-11-02T20:56:21.819363Z"
    },
    "papermill": {
     "duration": 0.035196,
     "end_time": "2025-11-02T20:56:21.821529",
     "exception": false,
     "start_time": "2025-11-02T20:56:21.786333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è NULL HANDLING WITH AVERAGES\n",
      "==================================================\n",
      "‚úÖ Loaded raw data: F:\\RetailSense_Lite\\data\\uploaded\\uploaded_data.csv\n",
      "üìä Shape: 525 rows √ó 16 columns\n",
      "\n",
      "üîç Remaining Missing Values Check:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "üíæ Intermediate dataset saved to: F:\\RetailSense_Lite\\data\\processed\\raw2.csv\n",
      "‚û°Ô∏è Use this file as input for Cell 4 (preprocessing).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Null Handling with Averages + Save Raw2 Dataset (Guarded)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if 'SKIP_PHASE1' in globals() and SKIP_PHASE1:\n",
    "    print(\"‚è≠Ô∏è Skipping null handling (no uploaded data).\")\n",
    "else:\n",
    "    print(\"üõ†Ô∏è NULL HANDLING WITH AVERAGES\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    raw_data_path = INPUT_CSV   # uploaded input\n",
    "    if not os.path.exists(raw_data_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Input CSV not found: {raw_data_path}\")\n",
    "\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "\n",
    "    print(f\"‚úÖ Loaded raw data: {raw_data_path}\")\n",
    "    print(f\"üìä Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            mean_val = df[col].mean()\n",
    "            df[col].fillna(mean_val, inplace=True)\n",
    "            print(f\"‚úÖ Filled NaN in numeric column '{col}' with mean ({mean_val:.2f})\")\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"‚úÖ Filled NaN in categorical column '{col}' with mode ('{mode_val}')\")\n",
    "\n",
    "    print(\"\\nüîç Remaining Missing Values Check:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    save_path = os.path.join(PROCESSED_DIR, \"raw2.csv\")\n",
    "\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"\\nüíæ Intermediate dataset saved to: {save_path}\")\n",
    "    print(\"‚û°Ô∏è Use this file as input for Cell 4 (preprocessing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ec9e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:21.844109Z",
     "iopub.status.busy": "2025-11-02T20:56:21.843670Z",
     "iopub.status.idle": "2025-11-02T20:56:21.891984Z",
     "shell.execute_reply": "2025-11-02T20:56:21.891018Z"
    },
    "papermill": {
     "duration": 0.061709,
     "end_time": "2025-11-02T20:56:21.893741",
     "exception": false,
     "start_time": "2025-11-02T20:56:21.832032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß DATA PREPROCESSING\n",
      "========================================\n",
      "üìÇ Loading intermediate dataset from Cell 3 (raw2.csv)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing complete!\n",
      "üìä Final shape: (525, 26)\n",
      "üíæ Cleaned dataset saved at: F:\\RetailSense_Lite\\data\\processed\\cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Phase 1 - Cell 4: Data Preprocessing & Save Cleaned Dataset (Guarded)\n",
    "# -------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if 'SKIP_PHASE1' in globals() and SKIP_PHASE1:\n",
    "    print(\"‚è≠Ô∏è Skipping preprocessing (no uploaded data).\")\n",
    "else:\n",
    "    print(\"üîß DATA PREPROCESSING\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    def preprocess_retail_data(df):\n",
    "        df = df.copy()\n",
    "        date_columns = ['week_start', 'week_end', 'expiry_date']\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        if 'week_start' in df.columns:\n",
    "            df['year'] = df['week_start'].dt.year\n",
    "            df['month'] = df['week_start'].dt.month\n",
    "        if 'category' in df.columns:\n",
    "            df['category_encoded'] = pd.Categorical(df['category']).codes\n",
    "        for col in ['season', 'weather']:\n",
    "            if col in df.columns:\n",
    "                dummies = pd.get_dummies(df[col], prefix=col)\n",
    "                df = pd.concat([df, dummies], axis=1)\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "        binary_columns = ['promotion', 'holiday_flag', 'disaster_flag', 'availability']\n",
    "        for col in binary_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0).astype(int)\n",
    "        if 'price' in df.columns and 'sales_qty' in df.columns:\n",
    "            df['revenue'] = df['price'] * df['sales_qty']\n",
    "        return df\n",
    "\n",
    "    print(\"üìÇ Loading intermediate dataset from Cell 3 (raw2.csv)...\")\n",
    "    input_path = os.path.join(PROCESSED_DIR, \"raw2.csv\")\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"‚ùå raw2.csv not found at: {input_path}\")\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "    df_cleaned = preprocess_retail_data(df)\n",
    "\n",
    "    print(f\"‚úÖ Preprocessing complete!\")\n",
    "    print(f\"üìä Final shape: {df_cleaned.shape}\")\n",
    "\n",
    "    save_path = os.path.join(PROCESSED_DIR, \"cleaned_data.csv\")\n",
    "    df_cleaned.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"üíæ Cleaned dataset saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8eeb4b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:21.917331Z",
     "iopub.status.busy": "2025-11-02T20:56:21.916970Z",
     "iopub.status.idle": "2025-11-02T20:56:22.675970Z",
     "shell.execute_reply": "2025-11-02T20:56:22.674733Z"
    },
    "papermill": {
     "duration": 0.773924,
     "end_time": "2025-11-02T20:56:22.677724",
     "exception": false,
     "start_time": "2025-11-02T20:56:21.903800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ INVENTORY ANALYSIS (Phase 1 - Cleaned Data)\n",
      "==================================================\n",
      "‚úÖ Loaded cleaned data: F:\\RetailSense_Lite\\data\\processed\\cleaned_data.csv\n",
      "üìä Records: 525, Columns: 26\n",
      "üìä Average Stock on Hand: 330.86 units\n",
      "üö® Low Stock Threshold (10th percentile): 214.80 units\n",
      "üìà Correlation (Stock vs Sales): 0.88\n",
      "‚ö†Ô∏è Items with Low Stock: 53\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Phase 1 - Cell 5: Inventory Analysis (Cleaned Data) - Robust Version (Guarded)\n",
    "# -------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if 'SKIP_PHASE1' in globals() and SKIP_PHASE1:\n",
    "    print(\"‚è≠Ô∏è Skipping inventory analysis (no uploaded data).\")\n",
    "else:\n",
    "    print(\"üì¶ INVENTORY ANALYSIS (Phase 1 - Cleaned Data)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    possible_paths = [\n",
    "        os.path.join(PROCESSED_DIR, \"cleaned_data.csv\"),\n",
    "        \"../data/processed/cleaned_data.csv\",\n",
    "        \"./data/processed/cleaned_data.csv\",\n",
    "    ]\n",
    "\n",
    "    DATA_PATH = None\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            DATA_PATH = p\n",
    "            break\n",
    "\n",
    "    if DATA_PATH is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"‚ùå cleaned_data.csv not found in expected locations.\\n\"\n",
    "            \"üëâ Please check your working directory or re-run Phase 1 preprocessing.\"\n",
    "        )\n",
    "\n",
    "    df_cleaned = pd.read_csv(DATA_PATH)\n",
    "    print(f\"‚úÖ Loaded cleaned data: {DATA_PATH}\")\n",
    "    print(f\"üìä Records: {df_cleaned.shape[0]}, Columns: {df_cleaned.shape[1]}\")\n",
    "\n",
    "    if 'stock_on_hand' in df_cleaned.columns:\n",
    "        avg_stock = df_cleaned['stock_on_hand'].mean()\n",
    "        low_stock_threshold = df_cleaned['stock_on_hand'].quantile(0.1)\n",
    "        corr_stock_sales = (\n",
    "            df_cleaned[['stock_on_hand','sales_qty']].corr().iloc[0,1]\n",
    "            if 'sales_qty' in df_cleaned.columns else np.nan\n",
    "        )\n",
    "\n",
    "        print(f\"üìä Average Stock on Hand: {avg_stock:.2f} units\")\n",
    "        print(f\"üö® Low Stock Threshold (10th percentile): {low_stock_threshold:.2f} units\")\n",
    "        if not np.isnan(corr_stock_sales):\n",
    "            print(f\"üìà Correlation (Stock vs Sales): {corr_stock_sales:.2f}\")\n",
    "\n",
    "        low_stock_items = df_cleaned[df_cleaned['stock_on_hand'] <= low_stock_threshold]\n",
    "        print(f\"‚ö†Ô∏è Items with Low Stock: {len(low_stock_items)}\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes[0,0].hist(df_cleaned['stock_on_hand'], bins=40, alpha=0.7, color='teal')\n",
    "        axes[0,0].axvline(low_stock_threshold, color='red', linestyle='--', \n",
    "                         label=f'Low Stock Threshold ({low_stock_threshold:.0f})')\n",
    "        axes[0,0].set_title(\"Stock on Hand Distribution\")\n",
    "        axes[0,0].set_xlabel(\"Stock Quantity\")\n",
    "        axes[0,0].set_ylabel(\"Frequency\")\n",
    "        axes[0,0].legend()\n",
    "\n",
    "        if 'sales_qty' in df_cleaned.columns:\n",
    "            df_cleaned['days_of_stock'] = df_cleaned['stock_on_hand'] / (df_cleaned['sales_qty'] + 1)\n",
    "            axes[0,1].hist(df_cleaned['days_of_stock'], bins=40, alpha=0.7, color='orange')\n",
    "            axes[0,1].set_title(\"Days of Stock Distribution (log scale)\")\n",
    "            axes[0,1].set_xlabel(\"Days of Stock\")\n",
    "            axes[0,1].set_ylabel(\"Frequency\")\n",
    "            axes[0,1].set_yscale(\"log\")\n",
    "        else:\n",
    "            axes[0,1].text(0.5, 0.5, \"No sales_qty column\", ha='center', va='center')\n",
    "            axes[0,1].set_title(\"Unavailable Metric\")\n",
    "\n",
    "        if 'year' in df_cleaned.columns and 'month' in df_cleaned.columns:\n",
    "            monthly_stock = df_cleaned.groupby(['year','month'])['stock_on_hand'].mean().reset_index()\n",
    "            for y in sorted(monthly_stock['year'].unique()):\n",
    "                year_data = monthly_stock[monthly_stock['year']==y]\n",
    "                smoothed = year_data['stock_on_hand'].rolling(2, center=True).mean()\n",
    "                axes[1,0].plot(year_data['month'], smoothed, marker='o', label=str(y))\n",
    "            axes[1,0].set_title(\"Monthly Avg Stock Trend (Smoothed)\")\n",
    "            axes[1,0].set_xticks(range(1,13))\n",
    "            axes[1,0].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "            axes[1,0].legend()\n",
    "        else:\n",
    "            axes[1,0].text(0.5, 0.5, \"No year/month columns\", ha='center', va='center')\n",
    "            axes[1,0].set_title(\"Unavailable Trend\")\n",
    "\n",
    "        if 'sales_qty' in df_cleaned.columns:\n",
    "            sample = df_cleaned.sample(min(2000,len(df_cleaned)), random_state=42)\n",
    "            scatter = axes[1,1].scatter(\n",
    "                sample['stock_on_hand'], sample['sales_qty'], \n",
    "                alpha=0.4,\n",
    "                c=sample['year'] if 'year' in sample.columns else 'blue',\n",
    "                cmap=\"coolwarm\" if 'year' in sample.columns else None\n",
    "            )\n",
    "            axes[1,1].set_title(\"Stock on Hand vs Sales Quantity\")\n",
    "            axes[1,1].set_xlabel(\"Stock on Hand\")\n",
    "            axes[1,1].set_ylabel(\"Sales Quantity\")\n",
    "            if 'year' in sample.columns:\n",
    "                legend1 = axes[1,1].legend(*scatter.legend_elements(), title=\"Year\")\n",
    "                axes[1,1].add_artist(legend1)\n",
    "        else:\n",
    "            axes[1,1].text(0.5, 0.5, \"No sales_qty column\", ha='center', va='center')\n",
    "            axes[1,1].set_title(\"Unavailable Plot\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No 'stock_on_hand' column found in cleaned data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f12636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:22.703080Z",
     "iopub.status.busy": "2025-11-02T20:56:22.702800Z",
     "iopub.status.idle": "2025-11-02T20:56:22.730691Z",
     "shell.execute_reply": "2025-11-02T20:56:22.729944Z"
    },
    "papermill": {
     "duration": 0.044276,
     "end_time": "2025-11-02T20:56:22.732150",
     "exception": false,
     "start_time": "2025-11-02T20:56:22.687874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° BUSINESS INSIGHTS\n",
      "==================================================\n",
      "‚úÖ Loaded cleaned data: F:\\RetailSense_Lite\\data\\processed\\cleaned_data.csv\n",
      "üìä Records: 525, Columns: 26\n",
      "\n",
      "üè∑Ô∏è CATEGORY PERFORMANCE:\n",
      "                   sum    mean\n",
      "category                      \n",
      "Dairy          23320.5  222.10\n",
      "Fruits         22877.2  217.88\n",
      "Groceries      21442.0  204.21\n",
      "Bakery         20410.7  194.39\n",
      "Personal Care  20402.8  194.31\n",
      "\n",
      "üéØ PROMOTION EFFECTIVENESS:\n",
      "Average Sales by Promotion Flag:\n",
      "promotion\n",
      "0    203.81\n",
      "1    253.93\n",
      "Name: sales_qty, dtype: float64\n",
      "\n",
      "üåü SEASONAL PATTERNS:\n",
      "season\n",
      "Fall      202.21\n",
      "Spring    205.34\n",
      "Summer    222.01\n",
      "Winter    196.92\n",
      "Name: sales_qty, dtype: float64\n",
      "\n",
      "üéâ HOLIDAY EFFECT:\n",
      "holiday_flag\n",
      "Non-Holiday    200.29\n",
      "Holiday        297.41\n",
      "Name: sales_qty, dtype: float64\n",
      "\n",
      "üí∞ PRICE ELASTICITY:\n",
      "Correlation (Price vs Sales): -0.009\n",
      "‚û°Ô∏è Weak correlation: Price not a major factor.\n",
      "\n",
      "üéØ KEY RECOMMENDATIONS\n",
      "==============================\n",
      "1. Focus on Dairy (top selling category).\n",
      "2. Promotions are effective (24.6% uplift). Increase promotional activities.\n",
      "3. Holiday sales stronger (48.5% higher). Plan inventory accordingly.\n",
      "4. Continue monitoring price, promotions, and seasonality trends.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Phase 1 - Cell 6: Business Insights from Cleaned Data (Robust Version) (Guarded)\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if 'SKIP_PHASE1' in globals() and SKIP_PHASE1:\n",
    "    print(\"‚è≠Ô∏è Skipping business insights (no uploaded data).\")\n",
    "else:\n",
    "    print(\"üí° BUSINESS INSIGHTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    possible_paths = [\n",
    "        os.path.join(PROCESSED_DIR, \"cleaned_data.csv\"),\n",
    "        \"../data/processed/cleaned_data.csv\",\n",
    "        \"./data/processed/cleaned_data.csv\",\n",
    "    ]\n",
    "\n",
    "    DATA_PATH = None\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            DATA_PATH = p\n",
    "            break\n",
    "\n",
    "    if DATA_PATH is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"‚ùå cleaned_data.csv not found in expected locations.\\n\"\n",
    "            \"üëâ Please check your working directory or re-run Phase 1 preprocessing.\"\n",
    "        )\n",
    "\n",
    "    df_cleaned = pd.read_csv(DATA_PATH)\n",
    "    print(f\"‚úÖ Loaded cleaned data: {DATA_PATH}\")\n",
    "    print(f\"üìä Records: {df_cleaned.shape[0]}, Columns: {df_cleaned.shape[1]}\")\n",
    "\n",
    "    def safe_division(numerator, denominator):\n",
    "        return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    def check_column_exists(df, columns):\n",
    "        if isinstance(columns, str):\n",
    "            return columns in df.columns\n",
    "        return all(col in df.columns for col in columns)\n",
    "\n",
    "    def get_year_range(df, year_col='year'):\n",
    "        if year_col in df.columns:\n",
    "            df[year_col] = df[year_col].astype(int)\n",
    "            return sorted(df[year_col].unique())\n",
    "        return []\n",
    "\n",
    "    print(\"\\nüè∑Ô∏è CATEGORY PERFORMANCE:\")\n",
    "    if check_column_exists(df_cleaned, 'category'):\n",
    "        category_perf = df_cleaned.groupby('category')['sales_qty'].agg(['sum','mean']).round(2)\n",
    "        category_perf = category_perf.sort_values('sum', ascending=False)\n",
    "        print(category_perf.head(10))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No 'category' column found.\")\n",
    "\n",
    "    print(\"\\nüéØ PROMOTION EFFECTIVENESS:\")\n",
    "    if check_column_exists(df_cleaned, 'promotion'):\n",
    "        promo_effect = df_cleaned.groupby('promotion')['sales_qty'].mean().round(2)\n",
    "        print(\"Average Sales by Promotion Flag:\")\n",
    "        print(promo_effect)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No 'promotion' column found.\")\n",
    "\n",
    "    print(\"\\nüåü SEASONAL PATTERNS:\")\n",
    "    if 'season' not in df_cleaned.columns and 'month' in df_cleaned.columns:\n",
    "        season_map = {\n",
    "            12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "            3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "            6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "            9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "        }\n",
    "        df_cleaned['season'] = df_cleaned['month'].map(season_map)\n",
    "\n",
    "    if 'season' in df_cleaned.columns:\n",
    "        seasonal_perf = df_cleaned.groupby('season')['sales_qty'].mean().round(2)\n",
    "        print(seasonal_perf)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No 'season' or 'month' column found.\")\n",
    "\n",
    "    print(\"\\nüéâ HOLIDAY EFFECT:\")\n",
    "    if check_column_exists(df_cleaned, 'holiday_flag'):\n",
    "        holiday_effect = df_cleaned.groupby('holiday_flag')['sales_qty'].mean().round(2)\n",
    "        holiday_effect.index = holiday_effect.index.map({0: 'Non-Holiday', 1: 'Holiday'})\n",
    "        print(holiday_effect)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No 'holiday_flag' column found.\")\n",
    "\n",
    "    print(\"\\nüí∞ PRICE ELASTICITY:\")\n",
    "    if check_column_exists(df_cleaned, ['price', 'sales_qty']):\n",
    "        corr = df_cleaned['price'].corr(df_cleaned['sales_qty'])\n",
    "        print(f\"Correlation (Price vs Sales): {corr:.3f}\")\n",
    "        if corr < -0.2:\n",
    "            print(\"üìâ Higher prices reduce sales significantly.\")\n",
    "        elif corr > 0.2:\n",
    "            print(\"üìà Higher prices increase perceived value (premium positioning).\")\n",
    "        else:\n",
    "            print(\"‚û°Ô∏è Weak correlation: Price not a major factor.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Price or sales data not available.\")\n",
    "\n",
    "    print(\"\\nüéØ KEY RECOMMENDATIONS\")\n",
    "    print(\"=\" * 30)\n",
    "    recs = []\n",
    "    if 'category' in df_cleaned.columns:\n",
    "        top_cat = df_cleaned.groupby('category')['sales_qty'].sum().idxmax()\n",
    "        recs.append(f\"1. Focus on {top_cat} (top selling category).\")\n",
    "    if 'promotion' in df_cleaned.columns:\n",
    "        promo_effect = df_cleaned.groupby('promotion')['sales_qty'].mean()\n",
    "        if 1 in promo_effect.index and 0 in promo_effect.index:\n",
    "            uplift = safe_division(promo_effect[1] - promo_effect[0], promo_effect[0]) * 100\n",
    "            if uplift > 0:\n",
    "                recs.append(f\"2. Promotions are effective ({uplift:.1f}% uplift). Increase promotional activities.\")\n",
    "            else:\n",
    "                recs.append(\"2. Promotions not effective. Reassess promo strategy.\")\n",
    "    if 'holiday_flag' in df_cleaned.columns:\n",
    "        hol = df_cleaned.groupby('holiday_flag')['sales_qty'].mean()\n",
    "        if 1 in hol.index and 0 in hol.index:\n",
    "            uplift = safe_division(hol[1] - hol[0], hol[0]) * 100\n",
    "            if uplift > 0:\n",
    "                recs.append(f\"3. Holiday sales stronger ({uplift:.1f}% higher). Plan inventory accordingly.\")\n",
    "            else:\n",
    "                recs.append(\"3. Holiday sales not significantly different.\")\n",
    "    recs.append(\"4. Continue monitoring price, promotions, and seasonality trends.\")\n",
    "    for r in recs:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b58bc29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:56:22.757968Z",
     "iopub.status.busy": "2025-11-02T20:56:22.757419Z",
     "iopub.status.idle": "2025-11-02T20:56:23.245565Z",
     "shell.execute_reply": "2025-11-02T20:56:23.244761Z"
    },
    "papermill": {
     "duration": 0.502541,
     "end_time": "2025-11-02T20:56:23.246992",
     "exception": false,
     "start_time": "2025-11-02T20:56:22.744451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä FINAL EXPLORATORY DATA ANALYSIS (Phase 1)\n",
      "======================================================================\n",
      "‚úÖ Loaded cleaned data: F:\\RetailSense_Lite\\data\\processed\\cleaned_data.csv\n",
      "üìä Shape: 525 rows, 26 columns\n",
      "\n",
      "üîé Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 525 entries, 0 to 524\n",
      "Data columns (total 26 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   product_id        525 non-null    object \n",
      " 1   product_name      525 non-null    object \n",
      " 2   category          525 non-null    object \n",
      " 3   week_no           525 non-null    int64  \n",
      " 4   week_start        525 non-null    object \n",
      " 5   week_end          525 non-null    object \n",
      " 6   sales_qty         525 non-null    float64\n",
      " 7   stock_on_hand     525 non-null    float64\n",
      " 8   availability      525 non-null    int64  \n",
      " 9   price             525 non-null    float64\n",
      " 10  promotion         525 non-null    int64  \n",
      " 11  holiday_flag      525 non-null    int64  \n",
      " 12  disaster_flag     525 non-null    int64  \n",
      " 13  expiry_date       525 non-null    object \n",
      " 14  year              525 non-null    int64  \n",
      " 15  month             525 non-null    int64  \n",
      " 16  category_encoded  525 non-null    int64  \n",
      " 17  season_Autumn     525 non-null    bool   \n",
      " 18  season_Spring     525 non-null    bool   \n",
      " 19  season_Summer     525 non-null    bool   \n",
      " 20  season_Winter     525 non-null    bool   \n",
      " 21  weather_Cloudy    525 non-null    bool   \n",
      " 22  weather_Rainy     525 non-null    bool   \n",
      " 23  weather_Stormy    525 non-null    bool   \n",
      " 24  weather_Sunny     525 non-null    bool   \n",
      " 25  revenue           525 non-null    float64\n",
      "dtypes: bool(8), float64(4), int64(8), object(6)\n",
      "memory usage: 78.1+ KB\n",
      "None\n",
      "\n",
      "üìà Descriptive Statistics:\n",
      "               count unique         top freq        mean        std     min  \\\n",
      "product_id       525      5        P001  105         NaN        NaN     NaN   \n",
      "product_name     525      5        Milk  105         NaN        NaN     NaN   \n",
      "category         525      5       Dairy  105         NaN        NaN     NaN   \n",
      "week_no        525.0    NaN         NaN  NaN        53.0  30.338422     1.0   \n",
      "week_start       525    105  2023-10-30    5         NaN        NaN     NaN   \n",
      "week_end         525    105  2023-11-05    5         NaN        NaN     NaN   \n",
      "sales_qty      525.0    NaN         NaN  NaN  206.577524  81.391107    16.0   \n",
      "stock_on_hand  525.0    NaN         NaN  NaN  330.861333  91.935242   120.0   \n",
      "availability   525.0    NaN         NaN  NaN         1.0        0.0     1.0   \n",
      "price          525.0    NaN         NaN  NaN  171.130133  71.822027   50.01   \n",
      "promotion      525.0    NaN         NaN  NaN    0.055238   0.228662     0.0   \n",
      "holiday_flag   525.0    NaN         NaN  NaN    0.064762    0.24634     0.0   \n",
      "disaster_flag  525.0    NaN         NaN  NaN    0.026667   0.161261     0.0   \n",
      "expiry_date      525    387  2024-10-13    5         NaN        NaN     NaN   \n",
      "year           525.0    NaN         NaN  NaN  2024.32381   0.625404  2023.0   \n",
      "\n",
      "                  25%     50%     75%     max  \n",
      "product_id        NaN     NaN     NaN     NaN  \n",
      "product_name      NaN     NaN     NaN     NaN  \n",
      "category          NaN     NaN     NaN     NaN  \n",
      "week_no          27.0    53.0    79.0   105.0  \n",
      "week_start        NaN     NaN     NaN     NaN  \n",
      "week_end          NaN     NaN     NaN     NaN  \n",
      "sales_qty       148.0   201.0   260.0   605.0  \n",
      "stock_on_hand   273.0   330.0   383.0   767.0  \n",
      "availability      1.0     1.0     1.0     1.0  \n",
      "price          106.69  171.51  233.78  299.36  \n",
      "promotion         0.0     0.0     0.0     1.0  \n",
      "holiday_flag      0.0     0.0     0.0     1.0  \n",
      "disaster_flag     0.0     0.0     0.0     1.0  \n",
      "expiry_date       NaN     NaN     NaN     NaN  \n",
      "year           2024.0  2024.0  2025.0  2025.0  \n",
      "\n",
      "üö® Missing Value Check:\n",
      "‚úÖ No missing values\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Phase 1 - Cell 7: Final Exploratory Analysis with Visualizations\n",
    "# -------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print(\"üìä FINAL EXPLORATORY DATA ANALYSIS (Phase 1)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ----------------------------\n",
    "# Load cleaned dataset\n",
    "# ----------------------------\n",
    "DATA_PATH = os.path.join(PROCESSED_DIR, 'cleaned_data.csv')\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df_cleaned = pd.read_csv(DATA_PATH)\n",
    "    print(f\"‚úÖ Loaded cleaned data: {DATA_PATH}\")\n",
    "    print(f\"üìä Shape: {df_cleaned.shape[0]} rows, {df_cleaned.shape[1]} columns\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"‚ùå Cleaned data not found at {DATA_PATH}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Basic Dataset Overview\n",
    "# ----------------------------\n",
    "print(\"\\nüîé Dataset Overview:\")\n",
    "print(df_cleaned.info())\n",
    "\n",
    "print(\"\\nüìà Descriptive Statistics:\")\n",
    "print(df_cleaned.describe(include='all').transpose().head(15))\n",
    "\n",
    "# ----------------------------\n",
    "# Missing Value Check\n",
    "# ----------------------------\n",
    "print(\"\\nüö® Missing Value Check:\")\n",
    "missing_summary = df_cleaned.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0] if missing_summary.sum() > 0 else \"‚úÖ No missing values\")\n",
    "\n",
    "# ----------------------------\n",
    "# Visualizations\n",
    "# ----------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1Ô∏è‚É£ Correlation Heatmap\n",
    "numeric_cols = df_cleaned.select_dtypes(include=['number']).columns\n",
    "if len(numeric_cols) > 1:\n",
    "    corr_matrix = df_cleaned[numeric_cols].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, ax=axes[0,0])\n",
    "    axes[0,0].set_title(\"Correlation Heatmap (Numeric Features)\")\n",
    "else:\n",
    "    axes[0,0].text(0.5, 0.5, \"Not enough numeric columns\", ha=\"center\", va=\"center\")\n",
    "    axes[0,0].set_title(\"Correlation Heatmap\")\n",
    "\n",
    "# 2Ô∏è‚É£ Category Distribution\n",
    "if 'category' in df_cleaned.columns:\n",
    "    category_counts = df_cleaned['category'].value_counts().head(10)\n",
    "    axes[0,1].bar(category_counts.index, category_counts.values, color='skyblue')\n",
    "    axes[0,1].set_title(\"Top 10 Categories by Count\")\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[0,1].text(0.5, 0.5, \"No 'category' column\", ha=\"center\", va=\"center\")\n",
    "    axes[0,1].set_title(\"Category Distribution\")\n",
    "\n",
    "# 3Ô∏è‚É£ Top Stores or Products\n",
    "if 'store_id' in df_cleaned.columns:\n",
    "    top_stores = df_cleaned['store_id'].value_counts().head(10)\n",
    "    axes[1,0].bar(top_stores.index, top_stores.values, color='orange')\n",
    "    axes[1,0].set_title(\"Top 10 Stores by Records\")\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "elif 'product_name' in df_cleaned.columns:\n",
    "    top_products = df_cleaned['product_name'].value_counts().head(10)\n",
    "    axes[1,0].bar(top_products.index, top_products.values, color='orange')\n",
    "    axes[1,0].set_title(\"Top 10 Products by Records\")\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1,0].text(0.5, 0.5, \"No 'store_id' or 'product_name' column\", ha=\"center\", va=\"center\")\n",
    "    axes[1,0].set_title(\"Store/Product Distribution\")\n",
    "\n",
    "# 4Ô∏è‚É£ Monthly Trend\n",
    "if 'date' in df_cleaned.columns:\n",
    "    df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], errors='coerce')\n",
    "    monthly_counts = df_cleaned.groupby(df_cleaned['date'].dt.to_period(\"M\")).size()\n",
    "    monthly_counts.plot(ax=axes[1,1], marker='o', color='green')\n",
    "    axes[1,1].set_title(\"Monthly Records Trend\")\n",
    "    axes[1,1].set_ylabel(\"Count\")\n",
    "elif 'week_start' in df_cleaned.columns:\n",
    "    df_cleaned['week_start'] = pd.to_datetime(df_cleaned['week_start'], errors='coerce')\n",
    "    monthly_counts = df_cleaned.groupby(df_cleaned['week_start'].dt.to_period(\"M\")).size()\n",
    "    monthly_counts.plot(ax=axes[1,1], marker='o', color='green')\n",
    "    axes[1,1].set_title(\"Monthly Records Trend\")\n",
    "    axes[1,1].set_ylabel(\"Count\")\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, \"No 'date' or 'week_start' column\", ha=\"center\", va=\"center\")\n",
    "    axes[1,1].set_title(\"Monthly Trend\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retailsense_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.497733,
   "end_time": "2025-11-02T20:56:24.596094",
   "environment_variables": {},
   "exception": null,
   "input_path": "F:\\RetailSense_Lite\\notebooks\\phase1_eda.ipynb",
   "output_path": "F:\\RetailSense_Lite\\notebooks\\phase1_out.ipynb",
   "parameters": {
    "INPUT_CSV": "F:\\RetailSense_Lite\\data\\uploaded\\uploaded_data.csv",
    "RETAILSENSE_BASE_DIR": "F:\\RetailSense_Lite"
   },
   "start_time": "2025-11-02T20:56:14.098361",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}